{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day082_Dropout.ipynb.google_collaborator",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sam19980822/3rd-ML100Days/blob/master/Day082_Dropout_ipynb_google_collaborator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO0XsMwyjEpv",
        "colab_type": "text"
      },
      "source": [
        "## Work\n",
        "1. 請比較使用不同層數以及不同 Dropout rate 對訓練的效果\n",
        "2. 將 optimizer 改成使用 Adam 並加上適當的 dropout rate 檢視結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiVgZjYWjFA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import keras\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIOUqFb9i0HL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yeq86EO1i2g9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 資料前處理\n",
        "def preproc_x(x, flatten=True):\n",
        "    x = x / 255.\n",
        "    if flatten:\n",
        "        x = x.reshape((len(x), -1))\n",
        "    return x\n",
        "\n",
        "def preproc_y(y, num_classes=10):\n",
        "    if y.shape[-1] == 1:\n",
        "        y = keras.utils.to_categorical(y, num_classes)\n",
        "    return y    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqUbOey9i6xS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = train\n",
        "x_test, y_test = test\n",
        "\n",
        "# 資料前處理 - X 標準化\n",
        "x_train = preproc_x(x_train)\n",
        "x_test = preproc_x(x_test)\n",
        "\n",
        "# 資料前處理 -Y 轉成 onehot\n",
        "y_train = preproc_y(y_train)\n",
        "y_test = preproc_y(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPp0UaB9i61z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "\"\"\"\n",
        "建立神經網路，並加入 dropout layer\n",
        "\"\"\"\n",
        "def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128], drp_ratio=0.2):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "    \n",
        "    for i, n_units in enumerate(num_neurons):\n",
        "        if i == 0:\n",
        "            x = keras.layers.Dense(units=n_units, \n",
        "                                   activation=\"relu\", \n",
        "                                   name=\"hidden_layer\"+str(i+1))(input_layer)\n",
        "            x = Dropout(drp_ratio)(x)\n",
        "        else:\n",
        "            x = keras.layers.Dense(units=n_units, \n",
        "                                   activation=\"relu\", \n",
        "                                   name=\"hidden_layer\"+str(i+1))(x)\n",
        "            x = Dropout(drp_ratio)(x)\n",
        "    \n",
        "    out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n",
        "    \n",
        "    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n",
        "    return model\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRPqsSb3i68F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 超參數設定\n",
        "LEARNING_RATE = 1e-3 #(Day80 HW 顯示1e-8的train 效果最好)\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 256\n",
        "Dropout_EXP = [0.3,0.4,0.5]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZnUG7-t7srR",
        "colab_type": "text"
      },
      "source": [
        "# 使用Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOe48fQnm84F",
        "colab_type": "code",
        "outputId": "8f3daee4-2243-47b1-fba7-5cda59ee707a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "使用迴圈建立不同的Drop_out Ratio\n",
        "\"\"\"\n",
        "\n",
        "for Dropout_Ratio in Dropout_EXP:\n",
        "    keras.backend.clear_session() # 把舊的 Graph 清掉\n",
        "    print(\"Experiment with Dropout_Ratio = %.6f\" % (Dropout_Ratio))\n",
        "    model = build_mlp(input_shape=x_train.shape[1:], drp_ratio=Dropout_Ratio)\n",
        "    model.summary()\n",
        "    optimizer = keras.optimizers.adam(lr=LEARNING_RATE)\n",
        "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n",
        "\n",
        "    model.fit(x_train, y_train, \n",
        "              epochs=EPOCHS, \n",
        "              batch_size=BATCH_SIZE, \n",
        "              validation_data=(x_test, y_test), \n",
        "              shuffle=True)\n",
        "    \n",
        "    # Collect results\n",
        "    train_loss = model.history.history[\"loss\"]\n",
        "    valid_loss = model.history.history[\"val_loss\"]\n",
        "    train_acc = model.history.history[\"acc\"]\n",
        "    valid_acc = model.history.history[\"val_acc\"]\n",
        "    \n",
        "    exp_name_tag = \"Dropout_Ratio%s\" % str(Dropout_Ratio)\n",
        "    results[exp_name_tag] = {'train-loss': train_loss,\n",
        "                             'valid-loss': valid_loss,\n",
        "                             'train-acc': train_acc,\n",
        "                             'valid-acc': valid_acc}\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment with Dropout_Ratio = 0.300000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,738,890\n",
            "Trainable params: 1,738,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 2s 41us/step - loss: 2.0763 - acc: 0.2303 - val_loss: 1.8640 - val_acc: 0.3244\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.9157 - acc: 0.3016 - val_loss: 1.7821 - val_acc: 0.3685\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8494 - acc: 0.3256 - val_loss: 1.7394 - val_acc: 0.3740\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8271 - acc: 0.3334 - val_loss: 1.7206 - val_acc: 0.3878\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8065 - acc: 0.3442 - val_loss: 1.7415 - val_acc: 0.3743\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7926 - acc: 0.3476 - val_loss: 1.6950 - val_acc: 0.4102\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.7719 - acc: 0.3554 - val_loss: 1.7021 - val_acc: 0.4019\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7541 - acc: 0.3638 - val_loss: 1.6999 - val_acc: 0.3996\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7456 - acc: 0.3682 - val_loss: 1.6663 - val_acc: 0.4070\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7449 - acc: 0.3646 - val_loss: 1.6653 - val_acc: 0.4162\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7194 - acc: 0.3770 - val_loss: 1.6556 - val_acc: 0.4204\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7061 - acc: 0.3788 - val_loss: 1.6344 - val_acc: 0.4237\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7013 - acc: 0.3852 - val_loss: 1.6727 - val_acc: 0.4141\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.6978 - acc: 0.3875 - val_loss: 1.6250 - val_acc: 0.4250\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 2s 32us/step - loss: 1.6895 - acc: 0.3904 - val_loss: 1.6406 - val_acc: 0.4252\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.6842 - acc: 0.3925 - val_loss: 1.6155 - val_acc: 0.4353\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.6736 - acc: 0.3964 - val_loss: 1.6006 - val_acc: 0.4362\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.6713 - acc: 0.3955 - val_loss: 1.6142 - val_acc: 0.4308\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.6625 - acc: 0.3962 - val_loss: 1.5889 - val_acc: 0.4443\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.6594 - acc: 0.4007 - val_loss: 1.5935 - val_acc: 0.4348\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.6536 - acc: 0.4010 - val_loss: 1.5916 - val_acc: 0.4384\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.6431 - acc: 0.4052 - val_loss: 1.5851 - val_acc: 0.4406\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.6445 - acc: 0.4036 - val_loss: 1.5737 - val_acc: 0.4481\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.6353 - acc: 0.4067 - val_loss: 1.5530 - val_acc: 0.4449\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.6391 - acc: 0.4062 - val_loss: 1.5761 - val_acc: 0.4452\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.6293 - acc: 0.4116 - val_loss: 1.5569 - val_acc: 0.4528\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.6208 - acc: 0.4111 - val_loss: 1.5352 - val_acc: 0.4603\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 2s 32us/step - loss: 1.6173 - acc: 0.4134 - val_loss: 1.5535 - val_acc: 0.4564\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.6191 - acc: 0.4132 - val_loss: 1.5403 - val_acc: 0.4586\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.6110 - acc: 0.4194 - val_loss: 1.5638 - val_acc: 0.4495\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 2s 35us/step - loss: 1.6138 - acc: 0.4203 - val_loss: 1.5438 - val_acc: 0.4505\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.6045 - acc: 0.4215 - val_loss: 1.5304 - val_acc: 0.4590\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.6021 - acc: 0.4227 - val_loss: 1.5512 - val_acc: 0.4439\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.5957 - acc: 0.4241 - val_loss: 1.5684 - val_acc: 0.4519\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.5968 - acc: 0.4230 - val_loss: 1.5530 - val_acc: 0.4544\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.5926 - acc: 0.4244 - val_loss: 1.5417 - val_acc: 0.4561\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.5891 - acc: 0.4255 - val_loss: 1.5371 - val_acc: 0.4621\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.5849 - acc: 0.4277 - val_loss: 1.5439 - val_acc: 0.4496\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.5851 - acc: 0.4276 - val_loss: 1.5279 - val_acc: 0.4582\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.5837 - acc: 0.4273 - val_loss: 1.5318 - val_acc: 0.4583\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.5794 - acc: 0.4289 - val_loss: 1.5586 - val_acc: 0.4542\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.5825 - acc: 0.4279 - val_loss: 1.5148 - val_acc: 0.4614\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.5819 - acc: 0.4271 - val_loss: 1.5592 - val_acc: 0.4495\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.5717 - acc: 0.4322 - val_loss: 1.5481 - val_acc: 0.4522\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.5748 - acc: 0.4313 - val_loss: 1.5127 - val_acc: 0.4689\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.5681 - acc: 0.4340 - val_loss: 1.5189 - val_acc: 0.4681\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.5725 - acc: 0.4320 - val_loss: 1.5530 - val_acc: 0.4570\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.5711 - acc: 0.4318 - val_loss: 1.5198 - val_acc: 0.4630\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.5594 - acc: 0.4378 - val_loss: 1.5077 - val_acc: 0.4753\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.5600 - acc: 0.4369 - val_loss: 1.5537 - val_acc: 0.4530\n",
            "Experiment with Dropout_Ratio = 0.400000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,738,890\n",
            "Trainable params: 1,738,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 2s 40us/step - loss: 2.1532 - acc: 0.1908 - val_loss: 1.9651 - val_acc: 0.3003\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9948 - acc: 0.2530 - val_loss: 1.8688 - val_acc: 0.3264\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9434 - acc: 0.2777 - val_loss: 1.8930 - val_acc: 0.3212\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9209 - acc: 0.2884 - val_loss: 1.8702 - val_acc: 0.3456\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9013 - acc: 0.2941 - val_loss: 1.8848 - val_acc: 0.3284\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.8963 - acc: 0.3008 - val_loss: 1.8515 - val_acc: 0.3399\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8868 - acc: 0.3052 - val_loss: 1.8905 - val_acc: 0.3168\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.8853 - acc: 0.3076 - val_loss: 1.8235 - val_acc: 0.3527\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8634 - acc: 0.3139 - val_loss: 1.8254 - val_acc: 0.3592\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8466 - acc: 0.3232 - val_loss: 1.8198 - val_acc: 0.3562\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.8468 - acc: 0.3238 - val_loss: 1.8157 - val_acc: 0.3658\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.8421 - acc: 0.3250 - val_loss: 1.8033 - val_acc: 0.3676\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8309 - acc: 0.3276 - val_loss: 1.8003 - val_acc: 0.3628\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.8254 - acc: 0.3305 - val_loss: 1.7825 - val_acc: 0.3841\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.8218 - acc: 0.3313 - val_loss: 1.8019 - val_acc: 0.3688\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8238 - acc: 0.3322 - val_loss: 1.7838 - val_acc: 0.3713\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8101 - acc: 0.3370 - val_loss: 1.7751 - val_acc: 0.3724\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8109 - acc: 0.3389 - val_loss: 1.7928 - val_acc: 0.3738\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8125 - acc: 0.3348 - val_loss: 1.7855 - val_acc: 0.3769\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8005 - acc: 0.3410 - val_loss: 1.7966 - val_acc: 0.3744\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7978 - acc: 0.3414 - val_loss: 1.7644 - val_acc: 0.3773\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7946 - acc: 0.3434 - val_loss: 1.7939 - val_acc: 0.3789\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8043 - acc: 0.3406 - val_loss: 1.7718 - val_acc: 0.3783\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7998 - acc: 0.3411 - val_loss: 1.7715 - val_acc: 0.3846\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 2s 32us/step - loss: 1.7922 - acc: 0.3480 - val_loss: 1.7598 - val_acc: 0.3812\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7847 - acc: 0.3468 - val_loss: 1.7609 - val_acc: 0.3701\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7813 - acc: 0.3488 - val_loss: 1.7692 - val_acc: 0.3791\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7867 - acc: 0.3457 - val_loss: 1.7583 - val_acc: 0.3942\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7832 - acc: 0.3485 - val_loss: 1.7695 - val_acc: 0.3771\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.7782 - acc: 0.3516 - val_loss: 1.7720 - val_acc: 0.3737\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 2s 32us/step - loss: 1.7830 - acc: 0.3506 - val_loss: 1.7499 - val_acc: 0.3867\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7754 - acc: 0.3536 - val_loss: 1.7675 - val_acc: 0.3746\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.7777 - acc: 0.3522 - val_loss: 1.7633 - val_acc: 0.3841\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.7673 - acc: 0.3547 - val_loss: 1.7555 - val_acc: 0.3772\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7754 - acc: 0.3545 - val_loss: 1.7430 - val_acc: 0.3892\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7698 - acc: 0.3546 - val_loss: 1.7366 - val_acc: 0.3913\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.7682 - acc: 0.3560 - val_loss: 1.7870 - val_acc: 0.3751\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7676 - acc: 0.3559 - val_loss: 1.7140 - val_acc: 0.3926\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7604 - acc: 0.3591 - val_loss: 1.7378 - val_acc: 0.3939\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7595 - acc: 0.3605 - val_loss: 1.7304 - val_acc: 0.3918\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7532 - acc: 0.3634 - val_loss: 1.7503 - val_acc: 0.3856\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7614 - acc: 0.3596 - val_loss: 1.7608 - val_acc: 0.3856\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7575 - acc: 0.3616 - val_loss: 1.7287 - val_acc: 0.4001\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7519 - acc: 0.3625 - val_loss: 1.7223 - val_acc: 0.4056\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7595 - acc: 0.3586 - val_loss: 1.7444 - val_acc: 0.3916\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7490 - acc: 0.3631 - val_loss: 1.7307 - val_acc: 0.4006\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.7536 - acc: 0.3628 - val_loss: 1.7250 - val_acc: 0.3918\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.7621 - acc: 0.3552 - val_loss: 1.7322 - val_acc: 0.3995\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.7548 - acc: 0.3609 - val_loss: 1.7178 - val_acc: 0.3970\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.7463 - acc: 0.3654 - val_loss: 1.7361 - val_acc: 0.3888\n",
            "Experiment with Dropout_Ratio = 0.500000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,738,890\n",
            "Trainable params: 1,738,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 2s 41us/step - loss: 2.2159 - acc: 0.1601 - val_loss: 2.0436 - val_acc: 0.2398\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 2.0838 - acc: 0.2011 - val_loss: 2.0148 - val_acc: 0.2681\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 2.0476 - acc: 0.2180 - val_loss: 1.9599 - val_acc: 0.2758\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 2s 32us/step - loss: 2.0194 - acc: 0.2305 - val_loss: 1.9775 - val_acc: 0.2576\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 2.0164 - acc: 0.2357 - val_loss: 2.0018 - val_acc: 0.2832\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 2.0055 - acc: 0.2377 - val_loss: 1.9876 - val_acc: 0.2694\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 2s 32us/step - loss: 1.9934 - acc: 0.2408 - val_loss: 1.9671 - val_acc: 0.2779\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.9947 - acc: 0.2405 - val_loss: 1.9760 - val_acc: 0.2831\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9831 - acc: 0.2450 - val_loss: 2.0070 - val_acc: 0.2725\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9823 - acc: 0.2443 - val_loss: 1.9597 - val_acc: 0.2876\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9847 - acc: 0.2448 - val_loss: 1.9994 - val_acc: 0.2674\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9746 - acc: 0.2516 - val_loss: 1.9683 - val_acc: 0.2819\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9696 - acc: 0.2543 - val_loss: 1.9481 - val_acc: 0.2922\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9708 - acc: 0.2537 - val_loss: 1.9736 - val_acc: 0.2715\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9660 - acc: 0.2559 - val_loss: 1.9753 - val_acc: 0.2794\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9629 - acc: 0.2567 - val_loss: 1.9518 - val_acc: 0.2837\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9512 - acc: 0.2643 - val_loss: 1.9840 - val_acc: 0.2837\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9546 - acc: 0.2637 - val_loss: 1.9703 - val_acc: 0.2729\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9568 - acc: 0.2600 - val_loss: 1.9453 - val_acc: 0.2933\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9546 - acc: 0.2631 - val_loss: 1.9902 - val_acc: 0.2569\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.9513 - acc: 0.2644 - val_loss: 1.9693 - val_acc: 0.2870\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9490 - acc: 0.2650 - val_loss: 1.9819 - val_acc: 0.2747\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9466 - acc: 0.2670 - val_loss: 2.0128 - val_acc: 0.2523\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9499 - acc: 0.2686 - val_loss: 1.9558 - val_acc: 0.2888\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9444 - acc: 0.2687 - val_loss: 1.9973 - val_acc: 0.2852\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9438 - acc: 0.2714 - val_loss: 1.9728 - val_acc: 0.2771\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 2s 32us/step - loss: 1.9493 - acc: 0.2671 - val_loss: 1.9528 - val_acc: 0.2994\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 2s 32us/step - loss: 1.9368 - acc: 0.2742 - val_loss: 1.9644 - val_acc: 0.2842\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9312 - acc: 0.2763 - val_loss: 1.9428 - val_acc: 0.3011\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.9329 - acc: 0.2769 - val_loss: 1.9236 - val_acc: 0.3047\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9247 - acc: 0.2815 - val_loss: 1.9782 - val_acc: 0.2945\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9203 - acc: 0.2820 - val_loss: 1.9237 - val_acc: 0.3025\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 2s 32us/step - loss: 1.9261 - acc: 0.2803 - val_loss: 1.9654 - val_acc: 0.2967\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9177 - acc: 0.2861 - val_loss: 1.9894 - val_acc: 0.2815\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9166 - acc: 0.2837 - val_loss: 1.9651 - val_acc: 0.3000\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.9148 - acc: 0.2866 - val_loss: 1.9012 - val_acc: 0.3072\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9105 - acc: 0.2862 - val_loss: 1.9483 - val_acc: 0.2982\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9159 - acc: 0.2865 - val_loss: 1.9595 - val_acc: 0.3037\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9058 - acc: 0.2878 - val_loss: 1.9213 - val_acc: 0.3093\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9092 - acc: 0.2910 - val_loss: 1.9521 - val_acc: 0.3123\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9123 - acc: 0.2864 - val_loss: 1.9705 - val_acc: 0.2890\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9245 - acc: 0.2824 - val_loss: 1.9244 - val_acc: 0.3058\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9047 - acc: 0.2920 - val_loss: 1.9804 - val_acc: 0.2774\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9090 - acc: 0.2886 - val_loss: 1.9427 - val_acc: 0.3004\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8993 - acc: 0.2926 - val_loss: 1.9114 - val_acc: 0.2982\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.9047 - acc: 0.2940 - val_loss: 1.9294 - val_acc: 0.3005\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.9075 - acc: 0.2901 - val_loss: 1.9173 - val_acc: 0.3003\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 2s 34us/step - loss: 1.8981 - acc: 0.2955 - val_loss: 1.9462 - val_acc: 0.3080\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8978 - acc: 0.2954 - val_loss: 1.9377 - val_acc: 0.3000\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 2s 33us/step - loss: 1.8935 - acc: 0.2956 - val_loss: 1.9452 - val_acc: 0.3086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwBn1Ah75cF5",
        "colab_type": "text"
      },
      "source": [
        "#### ValAccuracy 來看，當Dropout rate = 0.4，預測準確率最好"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9OFFRRt5vwx",
        "colab_type": "text"
      },
      "source": [
        "# 以上都是用Adam optimizer 推出來的，如果我們使用其他分類器，標準會有差異嗎？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_qzPVWK6mZF",
        "colab_type": "text"
      },
      "source": [
        "# SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMDi3iREAZ-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = train\n",
        "x_test, y_test = test\n",
        "\n",
        "# 資料前處理 - X 標準化\n",
        "x_train = preproc_x(x_train)\n",
        "x_test = preproc_x(x_test)\n",
        "\n",
        "# 資料前處理 -Y 轉成 onehot\n",
        "y_train = preproc_y(y_train)\n",
        "y_test = preproc_y(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owb3fx345THM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "40c47f83-69bc-4fbd-8922-de517bc9f362"
      },
      "source": [
        "results = {}\n",
        "\"\"\"\n",
        "使用迴圈建立不同的Drop_out Ratio\n",
        "\"\"\"\n",
        "for Dropout_Ratio in Dropout_EXP:\n",
        "    keras.backend.clear_session() # 把舊的 Graph 清掉\n",
        "    print(\"Experiment with Dropout_Ratio = %.6f\" % (Dropout_Ratio))\n",
        "    model = build_mlp(input_shape=x_train.shape[1:])   \n",
        "    model.summary()\n",
        "    optimizer = keras.optimizers.SGD(lr=LEARNING_RATE,momentum=0.95,decay=0.01, nesterov=False)\n",
        "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n",
        "\n",
        "    model.fit(x_train, y_train, \n",
        "              epochs=EPOCHS, \n",
        "              batch_size=BATCH_SIZE, \n",
        "              validation_data=(x_test, y_test), \n",
        "              shuffle=True)\n",
        "    \n",
        "    # Collect results\n",
        "    train_loss = model.history.history[\"loss\"]\n",
        "    valid_loss = model.history.history[\"val_loss\"]\n",
        "    train_acc = model.history.history[\"acc\"]\n",
        "    valid_acc = model.history.history[\"val_acc\"]\n",
        "    \n",
        "    exp_name_tag = \"Dropout_Ratio%s\" % str(Dropout_Ratio)\n",
        "    results[exp_name_tag] = {'train-loss': train_loss,\n",
        "                             'valid-loss': valid_loss,\n",
        "                             'train-acc': train_acc,\n",
        "                             'valid-acc': valid_acc}\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment with Dropout_Ratio = 0.300000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,738,890\n",
            "Trainable params: 1,738,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 2s 35us/step - loss: 2.1805 - acc: 0.1928 - val_loss: 2.0100 - val_acc: 0.2872\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 1s 29us/step - loss: 2.0384 - acc: 0.2536 - val_loss: 1.9440 - val_acc: 0.3096\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.9957 - acc: 0.2715 - val_loss: 1.9143 - val_acc: 0.3242\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.9778 - acc: 0.2792 - val_loss: 1.8973 - val_acc: 0.3297\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.9593 - acc: 0.2878 - val_loss: 1.8847 - val_acc: 0.3344\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.9465 - acc: 0.2922 - val_loss: 1.8753 - val_acc: 0.3370\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.9377 - acc: 0.2968 - val_loss: 1.8670 - val_acc: 0.3400\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.9336 - acc: 0.2992 - val_loss: 1.8615 - val_acc: 0.3447\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.9261 - acc: 0.3014 - val_loss: 1.8551 - val_acc: 0.3460\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9169 - acc: 0.3064 - val_loss: 1.8502 - val_acc: 0.3485\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9173 - acc: 0.3074 - val_loss: 1.8462 - val_acc: 0.3532\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9120 - acc: 0.3074 - val_loss: 1.8420 - val_acc: 0.3513\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.9089 - acc: 0.3105 - val_loss: 1.8383 - val_acc: 0.3519\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9026 - acc: 0.3133 - val_loss: 1.8357 - val_acc: 0.3545\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8997 - acc: 0.3131 - val_loss: 1.8326 - val_acc: 0.3547\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8994 - acc: 0.3154 - val_loss: 1.8301 - val_acc: 0.3566\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8962 - acc: 0.3142 - val_loss: 1.8268 - val_acc: 0.3561\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8925 - acc: 0.3200 - val_loss: 1.8249 - val_acc: 0.3584\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8897 - acc: 0.3209 - val_loss: 1.8231 - val_acc: 0.3596\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8864 - acc: 0.3213 - val_loss: 1.8204 - val_acc: 0.3591\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8865 - acc: 0.3204 - val_loss: 1.8185 - val_acc: 0.3598\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8845 - acc: 0.3191 - val_loss: 1.8169 - val_acc: 0.3603\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8839 - acc: 0.3221 - val_loss: 1.8155 - val_acc: 0.3611\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8801 - acc: 0.3233 - val_loss: 1.8132 - val_acc: 0.3617\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8801 - acc: 0.3243 - val_loss: 1.8118 - val_acc: 0.3632\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8770 - acc: 0.3254 - val_loss: 1.8102 - val_acc: 0.3636\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8761 - acc: 0.3239 - val_loss: 1.8084 - val_acc: 0.3639\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8755 - acc: 0.3225 - val_loss: 1.8075 - val_acc: 0.3643\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8751 - acc: 0.3232 - val_loss: 1.8060 - val_acc: 0.3648\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8727 - acc: 0.3277 - val_loss: 1.8050 - val_acc: 0.3651\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8716 - acc: 0.3266 - val_loss: 1.8039 - val_acc: 0.3670\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8704 - acc: 0.3271 - val_loss: 1.8022 - val_acc: 0.3676\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8701 - acc: 0.3260 - val_loss: 1.8014 - val_acc: 0.3682\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8692 - acc: 0.3287 - val_loss: 1.8003 - val_acc: 0.3682\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8698 - acc: 0.3286 - val_loss: 1.7996 - val_acc: 0.3678\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8655 - acc: 0.3312 - val_loss: 1.7980 - val_acc: 0.3675\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8672 - acc: 0.3297 - val_loss: 1.7970 - val_acc: 0.3677\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8638 - acc: 0.3294 - val_loss: 1.7960 - val_acc: 0.3677\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8655 - acc: 0.3266 - val_loss: 1.7952 - val_acc: 0.3679\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8618 - acc: 0.3313 - val_loss: 1.7940 - val_acc: 0.3689\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8636 - acc: 0.3275 - val_loss: 1.7933 - val_acc: 0.3700\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8603 - acc: 0.3336 - val_loss: 1.7923 - val_acc: 0.3696\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8598 - acc: 0.3330 - val_loss: 1.7916 - val_acc: 0.3705\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8619 - acc: 0.3313 - val_loss: 1.7908 - val_acc: 0.3709\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8576 - acc: 0.3306 - val_loss: 1.7899 - val_acc: 0.3724\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8552 - acc: 0.3335 - val_loss: 1.7889 - val_acc: 0.3720\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8563 - acc: 0.3327 - val_loss: 1.7883 - val_acc: 0.3719\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8565 - acc: 0.3335 - val_loss: 1.7876 - val_acc: 0.3730\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8537 - acc: 0.3331 - val_loss: 1.7869 - val_acc: 0.3729\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8562 - acc: 0.3293 - val_loss: 1.7863 - val_acc: 0.3729\n",
            "Experiment with Dropout_Ratio = 0.400000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,738,890\n",
            "Trainable params: 1,738,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 2s 32us/step - loss: 2.2089 - acc: 0.1906 - val_loss: 2.0494 - val_acc: 0.2886\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 2.0702 - acc: 0.2521 - val_loss: 1.9725 - val_acc: 0.2975\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 2.0220 - acc: 0.2683 - val_loss: 1.9404 - val_acc: 0.3111\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9926 - acc: 0.2801 - val_loss: 1.9183 - val_acc: 0.3179\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.9771 - acc: 0.2863 - val_loss: 1.9024 - val_acc: 0.3237\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9661 - acc: 0.2919 - val_loss: 1.8915 - val_acc: 0.3254\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9558 - acc: 0.2941 - val_loss: 1.8827 - val_acc: 0.3283\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.9490 - acc: 0.2998 - val_loss: 1.8763 - val_acc: 0.3324\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9383 - acc: 0.3022 - val_loss: 1.8701 - val_acc: 0.3374\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9340 - acc: 0.3057 - val_loss: 1.8649 - val_acc: 0.3392\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9273 - acc: 0.3080 - val_loss: 1.8590 - val_acc: 0.3416\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9234 - acc: 0.3080 - val_loss: 1.8552 - val_acc: 0.3395\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9183 - acc: 0.3113 - val_loss: 1.8508 - val_acc: 0.3411\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9126 - acc: 0.3106 - val_loss: 1.8473 - val_acc: 0.3436\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9086 - acc: 0.3156 - val_loss: 1.8450 - val_acc: 0.3420\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9057 - acc: 0.3156 - val_loss: 1.8413 - val_acc: 0.3472\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.9035 - acc: 0.3165 - val_loss: 1.8388 - val_acc: 0.3474\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9047 - acc: 0.3169 - val_loss: 1.8373 - val_acc: 0.3486\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.9009 - acc: 0.3156 - val_loss: 1.8351 - val_acc: 0.3498\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8992 - acc: 0.3181 - val_loss: 1.8323 - val_acc: 0.3502\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8982 - acc: 0.3194 - val_loss: 1.8304 - val_acc: 0.3513\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8935 - acc: 0.3211 - val_loss: 1.8283 - val_acc: 0.3522\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8913 - acc: 0.3196 - val_loss: 1.8265 - val_acc: 0.3537\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8921 - acc: 0.3206 - val_loss: 1.8256 - val_acc: 0.3530\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8855 - acc: 0.3256 - val_loss: 1.8232 - val_acc: 0.3538\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8864 - acc: 0.3247 - val_loss: 1.8216 - val_acc: 0.3550\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8857 - acc: 0.3231 - val_loss: 1.8201 - val_acc: 0.3561\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8840 - acc: 0.3241 - val_loss: 1.8191 - val_acc: 0.3568\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 1s 29us/step - loss: 1.8806 - acc: 0.3292 - val_loss: 1.8174 - val_acc: 0.3576\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8834 - acc: 0.3235 - val_loss: 1.8161 - val_acc: 0.3569\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8833 - acc: 0.3265 - val_loss: 1.8148 - val_acc: 0.3574\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8784 - acc: 0.3281 - val_loss: 1.8138 - val_acc: 0.3581\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8776 - acc: 0.3301 - val_loss: 1.8124 - val_acc: 0.3585\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8752 - acc: 0.3303 - val_loss: 1.8115 - val_acc: 0.3593\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8749 - acc: 0.3295 - val_loss: 1.8102 - val_acc: 0.3585\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8763 - acc: 0.3295 - val_loss: 1.8091 - val_acc: 0.3590\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8724 - acc: 0.3288 - val_loss: 1.8082 - val_acc: 0.3608\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8701 - acc: 0.3308 - val_loss: 1.8071 - val_acc: 0.3607\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8717 - acc: 0.3275 - val_loss: 1.8061 - val_acc: 0.3607\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8702 - acc: 0.3301 - val_loss: 1.8050 - val_acc: 0.3612\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8685 - acc: 0.3319 - val_loss: 1.8040 - val_acc: 0.3612\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8669 - acc: 0.3329 - val_loss: 1.8030 - val_acc: 0.3628\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8674 - acc: 0.3317 - val_loss: 1.8022 - val_acc: 0.3624\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8669 - acc: 0.3322 - val_loss: 1.8015 - val_acc: 0.3626\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8658 - acc: 0.3333 - val_loss: 1.8008 - val_acc: 0.3624\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8645 - acc: 0.3324 - val_loss: 1.7997 - val_acc: 0.3642\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8650 - acc: 0.3312 - val_loss: 1.7990 - val_acc: 0.3628\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8635 - acc: 0.3338 - val_loss: 1.7983 - val_acc: 0.3634\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8626 - acc: 0.3337 - val_loss: 1.7975 - val_acc: 0.3643\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8610 - acc: 0.3350 - val_loss: 1.7967 - val_acc: 0.3635\n",
            "Experiment with Dropout_Ratio = 0.500000\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,738,890\n",
            "Trainable params: 1,738,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 2s 31us/step - loss: 2.2079 - acc: 0.1815 - val_loss: 2.0436 - val_acc: 0.2809\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 2.0651 - acc: 0.2463 - val_loss: 1.9646 - val_acc: 0.3146\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 2.0152 - acc: 0.2660 - val_loss: 1.9250 - val_acc: 0.3264\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9856 - acc: 0.2774 - val_loss: 1.9034 - val_acc: 0.3300\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.9680 - acc: 0.2867 - val_loss: 1.8892 - val_acc: 0.3364\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.9529 - acc: 0.2928 - val_loss: 1.8761 - val_acc: 0.3411\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9428 - acc: 0.2973 - val_loss: 1.8672 - val_acc: 0.3424\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.9360 - acc: 0.3006 - val_loss: 1.8600 - val_acc: 0.3461\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.9280 - acc: 0.3059 - val_loss: 1.8534 - val_acc: 0.3473\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.9229 - acc: 0.3069 - val_loss: 1.8485 - val_acc: 0.3480\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.9183 - acc: 0.3045 - val_loss: 1.8448 - val_acc: 0.3475\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9113 - acc: 0.3114 - val_loss: 1.8406 - val_acc: 0.3523\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.9086 - acc: 0.3120 - val_loss: 1.8359 - val_acc: 0.3535\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.9047 - acc: 0.3120 - val_loss: 1.8330 - val_acc: 0.3537\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.9007 - acc: 0.3162 - val_loss: 1.8295 - val_acc: 0.3566\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8976 - acc: 0.3160 - val_loss: 1.8264 - val_acc: 0.3570\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8959 - acc: 0.3183 - val_loss: 1.8248 - val_acc: 0.3577\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8909 - acc: 0.3177 - val_loss: 1.8223 - val_acc: 0.3581\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8887 - acc: 0.3190 - val_loss: 1.8193 - val_acc: 0.3587\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8886 - acc: 0.3204 - val_loss: 1.8172 - val_acc: 0.3574\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8888 - acc: 0.3225 - val_loss: 1.8160 - val_acc: 0.3594\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8850 - acc: 0.3233 - val_loss: 1.8135 - val_acc: 0.3578\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8830 - acc: 0.3233 - val_loss: 1.8117 - val_acc: 0.3584\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8810 - acc: 0.3238 - val_loss: 1.8104 - val_acc: 0.3600\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8777 - acc: 0.3236 - val_loss: 1.8084 - val_acc: 0.3598\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8780 - acc: 0.3217 - val_loss: 1.8068 - val_acc: 0.3611\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8768 - acc: 0.3253 - val_loss: 1.8053 - val_acc: 0.3617\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 2s 32us/step - loss: 1.8750 - acc: 0.3261 - val_loss: 1.8038 - val_acc: 0.3615\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 1s 29us/step - loss: 1.8727 - acc: 0.3271 - val_loss: 1.8032 - val_acc: 0.3620\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8714 - acc: 0.3277 - val_loss: 1.8019 - val_acc: 0.3627\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 1s 29us/step - loss: 1.8705 - acc: 0.3285 - val_loss: 1.8005 - val_acc: 0.3633\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8663 - acc: 0.3289 - val_loss: 1.7991 - val_acc: 0.3632\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8684 - acc: 0.3295 - val_loss: 1.7981 - val_acc: 0.3638\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8654 - acc: 0.3282 - val_loss: 1.7967 - val_acc: 0.3642\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8661 - acc: 0.3301 - val_loss: 1.7953 - val_acc: 0.3644\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8625 - acc: 0.3313 - val_loss: 1.7942 - val_acc: 0.3647\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8638 - acc: 0.3301 - val_loss: 1.7939 - val_acc: 0.3654\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 1s 26us/step - loss: 1.8608 - acc: 0.3316 - val_loss: 1.7925 - val_acc: 0.3651\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8610 - acc: 0.3325 - val_loss: 1.7911 - val_acc: 0.3663\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8642 - acc: 0.3308 - val_loss: 1.7909 - val_acc: 0.3658\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8574 - acc: 0.3349 - val_loss: 1.7895 - val_acc: 0.3661\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8601 - acc: 0.3300 - val_loss: 1.7888 - val_acc: 0.3667\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8580 - acc: 0.3319 - val_loss: 1.7880 - val_acc: 0.3670\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8590 - acc: 0.3317 - val_loss: 1.7871 - val_acc: 0.3669\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8584 - acc: 0.3320 - val_loss: 1.7863 - val_acc: 0.3667\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8530 - acc: 0.3366 - val_loss: 1.7853 - val_acc: 0.3681\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 1s 28us/step - loss: 1.8500 - acc: 0.3351 - val_loss: 1.7843 - val_acc: 0.3676\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8526 - acc: 0.3368 - val_loss: 1.7837 - val_acc: 0.3692\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8546 - acc: 0.3317 - val_loss: 1.7832 - val_acc: 0.3688\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 1s 27us/step - loss: 1.8532 - acc: 0.3341 - val_loss: 1.7823 - val_acc: 0.3686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QogkRzxQ6OLN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "779f9049-7188-41dc-af50-65ab172d48e9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "color_bar = [\"r\", \"g\", \"b\"]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "for i, cond in enumerate(results.keys()):\n",
        "    plt.plot(range(len(results[cond]['train-loss'])),results[cond]['train-loss'], '-', label=cond, color=color_bar[i])\n",
        "    plt.plot(range(len(results[cond]['valid-loss'])),results[cond]['valid-loss'], '--', label=cond, color=color_bar[i])\n",
        "plt.title(\"Loss\")\n",
        "plt.ylim([0, 5])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "for i, cond in enumerate(results.keys()):\n",
        "    plt.plot(range(len(results[cond]['train-acc'])),results[cond]['train-acc'], '-', label=cond, color=color_bar[i])\n",
        "    plt.plot(range(len(results[cond]['valid-acc'])),results[cond]['valid-acc'], '--', label=cond, color=color_bar[i])\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAAF1CAYAAABPriuUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIhUtCKiIiESEErIiISov8PhSmgGlXveeEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAF1CAYAAAAX0biNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOydd3hU1fa/353eE0ghQBoBgiT0Ih1U\nEBEUQYULKOjXhgWsV2yoP7vee+2CDVFB7IAF6SAgvbckJEAgJKQH0vvM/v2x0oDQEyCw3+c5z8yc\nus8E5nPW2qsorTUGg8FgMBjqFzYXewAGg8FgMBjOHiPgBoPBYDDUQ4yAGwwGg8FQDzECbjAYDAZD\nPcQIuMFgMBgM9RAj4AaDwWAw1EOMgBsMBoPBUA8xAm4wXAYopVYopY4qpRwv9lgMBsOFwQi4wVDP\nUUqFAH0ADQy9gNe1u1DXMhgMJ2IE3GCo/4wD1gPfAHdVrFRKOSul3lVKxSulspVSq5VSzuXbeiul\n1iqlspRSCUqpu8vXr1BK3VftHHcrpVZX+6yVUo8opfYCe8vXfVh+jhyl1BalVJ9q+9sqpZ5XSu1X\nSuWWbw9USk1RSr1b/SaUUn8opZ6oiy/IYLgcMQJuMNR/xgGzypcblFKNytf/D+gM9AQaApMAq1Iq\nGFgAfAz4Ah2A7WdxvWFANyC8/POm8nM0BL4HflFKOZVvexIYDQwGPIB7gALgW2C0UsoGQCnlAwwo\nP95gMJwBRsANhnqMUqo3EAz8rLXeAuwHxpQL4z3AY1rrw1pri9Z6rda6GBgDLNVa/6C1LtVaZ2qt\nz0bA39JaH9FaFwJorb8rP0eZ1vpdwBFoVb7vfcBkrXWMFnaU77sRyAb6l+83ClihtU49z6/EYLhi\nMAJuMNRv7gIWa60zyj9/X77OB3BCBP14Ak+y/kxJqP5BKfVvpVR0uZs+C/Asv/7prvUtcGf5+zuB\nmecxJoPhisMEoRgM9ZTy+eyRgK1SKqV8tSPgBTQGioDmwI7jDk0Arj7JafMBl2qf/WvYp7KFYfl8\n9yTEko7UWluVUkcBVe1azYHdNZznO2C3Uqo90Br47SRjMhgMNWAscIOh/jIMsCBz0R3Kl9bAP8i8\n+HTgPaVUk/Jgsh7laWazgAFKqZFKKTullLdSqkP5ObcDtyqlXJRSLYB7TzMGd6AMSAfslFIvIXPd\nFUwDXlNKtVRCO6WUN4DWOhGZP58JzK5wyRsMhjPDCLjBUH+5C/haa31Ia51SsQCfAHcAzwK7EJE8\nArwD2GitDyFBZU+Vr98OtC8/5/tACZCKuLhnnWYMi4CFQCwQj1j91V3s7wE/A4uBHOArwLna9m+B\nthj3ucFw1iit9en3MhgMhjpAKdUXcaUHa/NjZDCcFcYCNxgMFwWllD3wGDDNiLfBcPYYATcYDBcc\npVRrIAsJtvvgIg/HYKiXGBe6wWAwGAz1EGOBGwwGg8FQDzECbjAYDAZDPaReFXLx8fHRISEhF3sY\nBoPBYDBcELZs2ZKhtfataVu9EvCQkBA2b958sYdhMBgMBsMFQSkVf7JtxoVuMBgMBkM9xAi4wWAw\nGAz1ECPgBoPBYDDUQ4yAGwwGg8FQDzECbjAYDAZDPcQIuMFgMBgM9ZAzEnCl1CClVIxSap9S6tka\ntj+olNqllNqulFqtlAovX39H+bqKxVrRd1gptaL8nBXb/Gr31gwGg8FguHw5bR64UsoWmAJcDyQC\nm5RSf2ito6rt9r3W+rPy/YciPYAHaa1nUd5PWCnVFvhNa7292nF3aK1NYrfBYDAYDGfJmVjgVwP7\ntNZxWusS4Efgluo7aK1zqn10BWrqkDK6/FiDwWAwGAznyZlUYmsKJFT7nAh0O34npdQjwJOAA3Bd\nDef5F8cJP/C1UsoCzAZer6knsFLqAeABgKCgoDMYrsFgMBgMlz+1FsSmtZ6itW4OPANMrr5NKdUN\nKNBa7662+g6tdVugT/ky9iTn/UJr3UVr3cXXt8ZysAaDwWAwXHGciQV+GAis9jmgfN3J+BH49Lh1\no4Afqq/QWh8uf81VSn2PuOpnnMF4DAaDwWCofVJTYdcusLcHOzt5tbeHsDBwdYXcXMjIOPG4pk3B\nwQHy8sDN7YIN90wEfBPQUinVDBHuUcCY6jsopVpqrfeWfxwC7K22zQYYiVjZFevsAC+tdYZSyh64\nCVh6PjdiMBgMhsuUsjIRVIAtW8DTE3x85FWpsz9fcjJs2gSbN8vr//t/0K0b/PMPjBhx4v5r1kDP\nnvDrr3DPPSdu37kT2raF336DO+88+/GcI6cVcK11mVJqArAIsAWma60jlVKvApu11n8AE5RSA4BS\n4ChwV7VT9AUStNZx1dY5AovKxdsWEe8va+WODAaDwVC/KS6G1ath0SJYuBC6dIHp00Fr6N5dBB3A\n1hYaNoRHHoGXX4aiIrjtthMt6FtvhWHDICYG+veHw+VOZBsbiIiA7Gz5fM01sGqVnL+0tGq56irZ\n3rs3fPPNieMNCJDXHj3q8ls5gTNqJ6q1ng/MP27dS9XeP3aKY1cA3Y9blw90PpuBGgwGg+EyResq\nS/qhh2DGDCgoEPHt06dKGLUWKzczU5aMDHlt1Uq2l5WJG7y09FgR7tRJtgcFiUh36QJdu0LHjuDi\nUjUOHx+53slo2VKWk9G8+Tl/BeeCqiHw+5KlS5cu2vQDNxgMhnpCcTHs2QNRUTBypFjM330Hs2dX\niXBmpswd5+aKiL/2GqSkwKBBcO21F3RO+VJEKbVFa92lpm1nZIEbDAaD4Qrj6NGqOeJ9+8RN3bkz\nJCXJXLGPD3h7y+LpKUFetrawZAl89hlERspxFouc7/rr5ZikJFnv7S2uaW9vWW+xiNv7xRcv7n3X\nI4yAGwwGw5VOXp64mhs0EOEdNkxEtoLGjauCszZsgFGjTjzHpk3imk5Lg927ZW55xAh5jYiQcwNM\nmiTLJYTWMuQtW8DDQzzuISHyTGGxSID5pYgRcIPBYLiSKCuDZctg+3bYsUNeY2IkCOyllyQlqm1b\nibbu2lWs7grxBRg4UNSu+hx0VpYEkwHccYcs9YCsLDH4//wT4uOr1k+dKlPxu3dDhw7ixa9wNri5\nydfUv798be++C87OMpXu5QXPPHPhxm8E3GAwXLmUlUlKUUKCLEOHyq/xzJnw8cfyC1+djRvlV/rd\nd+HzzyWKuUGDql/3L74AR0exUhMSZJ2Xl5h1Hh5wNsWoysqgsFDmkUHc0xVCmpVVFYlttUr0tVIQ\nWF6yY80aEdbCQgkGi40VYZ4wQfYbPly2BQWJQo0YIXPOIOOdM+fk43J1FYv6EiEhQbz2K1fKV+Hv\nL1/Do4/K9uRkiYVr2BDS02H+fPmz3XWXiPFvv4nFPXmyxK8VForDAcSz/9prVc8qR45Afn5VvF1a\nGsybJ19xYaGczwi4wWAwnC8WiwRDJSRAYqK8jholv86//AJPPCG/7lZr1TGRkRAeXiWWzZsfm2dc\nkYvctKm4i8vKZK44KUkCtSp8rV9+CV99dex4XF3FVQ0yn7x4sYi6jY38+vv4wIoVsv3GGyV9qjoR\nEWISAgwZAmvXHru9R4+qdePHy71UH/eoUSLgtraSKhUaWmU11yPS02X4w4fLV/faa/J1+/mJFZyS\nIl9lhYDffz/89Zd8BRaLuMv79RMBt7ODgwflK6mJpk1F2E9Gnz7yp6+gYrr/QmGi0A0Gw+XHwoWS\n+1tYeOz6RYvEBbxmjfzqBwYeu4SF1c6EZ0aG/LJnZkqOcU6OzDHfe69snzpV8pyzs+UBwsVFHiw+\n+US2z5ghPl0XFxmPUiK2Y8praP36qygVyDYnJ2jSRIQfYNs2Oa+zsyxNmohnoBbQWp5ZbGzEWE9L\nE2fF0aNVt5qTI89HQ4fKvHKvXvKsUz09+4sv4PbbxVlx551ymxWuaGdneOUVSfnevVv2tVrlK9ux\nQ8axYwe0ayfOhZISeb5RSsZXUCDPSyDWeXS0fF2urvLs0779udV/uRicKgrdCLjBYKjfVPyyz5ol\nOb6jR4uqvPyy/FIHBkqhjcBAsarryy/3OWKxiOhlZ0PfvrJu4EDxsvv7Vy2dO8MNN8j22FhxD1eI\nb3a2pFb37i3PQA89JE6MCkdGQQG88QY8/7x8DgkRMff0FKeCpyc8+STccos4OT74QES7em2Uu+4S\nJ0ZUlJyruLjK419YKLMUvXqJy/uOO+S+unaVuef+/WX8dleAD9kIuMFguPyIjJS56h9+gEOHxHR7\n8UV49tmLPbLzorBQ3Lrx8VXT0qtXixVZIZAVIhkeLs8jK1fC0qXiQd+4UTz17dtLfBrAfffB/v1i\nhaakyBT6uHHw7bdisbq6nuisePhhmDJFLOdmzcSdXOGoCAgQN3TnznJ8RQaYofYxeeAGg+HSxGoV\n069CWXJzZXIT4KOPRJkqzMKcHLGgK+Z5H3tM5owHDoQ33xRz7xIt+lFQIAXCqlu4OTlw883g7g4/\n/ihu6Li4Ks84iFvay0ueUaZOPfacdnbiOgb49FPxqrdrJ5Ztjx7HVvWcNu3YY4uKZAH5E0yffuzD\ngYeHxN9VXCchgZOilBHvi4X52g0Gw/mjtfhAs7Ml0trGRkzGbduOLXt55IhU4rKxgX//Gz78sCqa\nGmQut6BAVCEuTqp4eXqKcIeEiBlYweTJ8P33Er1UR7dUYVlmZMD69ccWD8vMhMcfl1oky5fDCy9U\nuYAr3MCLF4uVOmsWPPDAideoiJmzWmUOePBgsXZDQ2WpqPL53//Cc88dK/4VXxOIu3natDN/fnFy\nkgUkgKumtO76yMJ9C3Gxd6F7QHccbM8vlsGqrcRnxePj4oO7oztL45by4t8v4uviS5h3WOXStUlX\nXB1ca+kOzg4j4AaDoWYSE6US1/Gq9cYb0KiRNHV47bUqVSktleOSkiQg66efJBKpgop0q/x8MTt7\n9RLVqj4x6+9ftf8HH5x6fNdcU+PqggIZelKSZEmFhsrQKrpEVix2dlXztocPS/xYxRxvReD6d99J\nhtX27WItV1DRQ2PECBFwe3u5pYpI6IpArIqsr379jrVyK16bNZPtY8ZUxafVRMU5T0b155orAYvV\nwj+H/uGn3T+x4fAGNj+wGRtlwzNLn2Fn6k5c7F3oE9SH/s36M6jFINo2anvS85RZy3C0cyQtP40Z\nO2YQmR5JZFokUelR5JfmM2fkHIa3Ho6bgxvOds4czDrIkrglFJWJC2Pb+G108O/AHzF/8FPkT8wc\nPhMbZXNBvgczB24wXOlYrRLSu3ChRGl/8gm0aSOR0HdVayzo4CACvGSJhPwuWCCmZXVF8vCAsWPl\nNTlZhN3bW5SsFvysVqsI84EDMk8cHCyBWrm5ktKTkCBGfgUvvyydIpOSaha5998XKzomRmqXNG1a\nFe8WGCjBU+3ayW3ExFSle1dkf13paK3ZnbYbOxs7Wvu2RmvNov2L8Hb2xtvFG29nbzwcPVDHBQ5a\ntZXC0kIKSgsoLCvE09ETTydPSi2lpBek42Lvgou9C/Y29sccG5UexdRNU/k16ldS81NxsXfhprCb\n+GzIZzRwbkBWURYrDq5gWdwylh1YRnRGNKPajOKH234A4MXlL7L3yF4SchJIzEkkKTeJ/17/Xx7v\n/jixmbG0+qQV/m7+RPhGEOEbQRu/NtzQ4gaCPINOGH9CdgKxmbH0Ce6Dk50T07ZO4+vtX7PmnjW1\n+h2bIDaDwXAihw+LX3bxYpmgBenO9OGHoobp6VXFSLy9JdKpjiO4i4slgDwlReLS3NwkUlprGVp0\ndNW8L8Ddd8PXX8v2226rKuIRECCZU82bi+e9wp1dEQFd0aiqSxcR6IpUcCPKp6e4rJiV8Sv5M+ZP\n5u2dx8Gsg4xrP45vh31LTnEOnm97HrO/nY0dr17zKs/1eY74rHhafdKKYkvxMft8NOgjJnabSGRa\nJG0+bVO53kbZ4GLvwqdDPuXOdneyYO8Cbvv5NoaEDWFk+EgGtxx8Svd1Um4SBaUFtGjYgoNZB2k9\npTUBHgEEegQS6BlIgHsAQ8KG0DOwJxarhZziHBo4Nzjp+S4GRsANhiuV9eurfMIVfuFBgyQfOSdH\n/L/XXisqOXDgsS7s01BSIhpfYdn+8IOkVycmigg7OIiF/O23sv2jj2Ra28VF5l+zssQwr+hd0a2b\nRFBX57rrpOonSGEOJ6eq+eGQEBHqU7mWDadHa41VW7FRNiilsGorx+tCUVlRpVC2/6w9O1N34mzn\nzPXNr+emljcxJGwITdybUGopZXPSZjIKMsgszCSzIJPMwkyua3YdA0IHkFOcwxur3sDZ3rnSyna2\nc6ZHYA/CfcPJLMjk16hfKSgtqLTOC0oLGN1mNF2bdqXMWkZRWRFuDucWrKi1PsEbcKljBNxguFzZ\nt08CvWJjxccbGytRUVOmyHZfX4nAArGgAwOlusY778i66n2YT8OOHVLRatUqeZ+aKvO+2dmyfcwY\nydkNDJQp8tJSufyvv8r24cMl2KuwULa5uMg0+OLFsv2DDyT9yd9fjm/aVCxoT8+ax3O5o7VmV9ou\nNidtxlbZMrjlYHxdfYnPimdn6k7sbe2xt7HH3tYehaJzk8642LtwOOcwcUfjqs6DJqsoixtb3Ii9\nrT0/R/7Mj7t/JCUvpXIpLCukeHIxDrYOTJg/gSmbphwzFh8XH1KeSsHWxpafI3/GzcGNa0Ouxdne\n+UJ/LVccJo3MYKiv7NlzbAJvSopER/3nP7K9IsIKxM0dFnasFT1njihgYCB4eaFRJCaC5WBFoJXC\nxeXEUpL5+WK8r1oljaNcXaX66BtvyPT4jTeKdR0YKO5nGxuZMj/VNPfcuVXvLZYTr/n44+f8LV1W\nJOUm8dnmz/g58mdiMmMq12+8byO+rr4s3r+YB+adGNIe9XAUrX1b83Pkzzy5+MkTth96/BCBnoEk\n5yYTmxmLv5s/PQN74u/mj4ejB7ZK/iBDWg6hkWujY451tnem2FKMi40LIyNG1vIdG84VY4EbDOeL\nxSLlpNaulXnlRo2kGljDhmJSKlVV1/F4Nm0SpYyLgwMHSIrNw1Xn4Rm9XrYPHy7dFirw8pLmE3//\nLZ9XrhRfdVhYVeLuSdi2TfR+//5j18+eLVVHly+XkpZOTuJtLysTYV67VtzbaWkiuqe5zBWD1pqC\n0oJaSSGKTo9Gown3DWdPxh4ipkbQL7gfIyNGMiB0ADbKhibuTXCycyKjIIODWQcptZRSai2l1CLR\n/90DuuPq4Mqh7EPszdx7zPk9nTxp69cWR7vaKadquHAYF7rBUJtkZYlouriIhXv33ejcXAAqndFx\ncZIj9M47UhnMzU0sY09Pic4+cEDOMXEi6Z/8iK2zIw1DvZjtdAejtk2i7zU23Hyz4qbm0bTwy6ny\nK1ck756GjAwJEp83T6a4H3xQioKMHSvT3W5uVbnKw4ZBixawc6cUEyksFMu6Xz/o2VMirq9kcotz\n2XtkL3FH47g9/HYAnl/2PFM2TSGnOAcfFx/CvMNo7dOaL2/+EqUUqXmpeDp54mQnf68yaxl5JXl4\nOXkB8HPkz+w/sp/kvGRWHFzBrrRdx0RLp+en4+t6Fp3LDJctRsANhvOgoAAyN8Xh/ctnuKyYT1Sk\n5odbfyHRI5yE6DwS9uSTWOjN+rnJtL2hCQt+ymHOck/6XWNDX4/tBEUvqnJ/Hz0KAQGUvPk/5q/1\n4pvPivhrmSMvvQgvvqTIzoa33hLhrWgmddVVYnAfH19WWCgBY0VFkgIFUkxkxQox6itaKz79tNSl\nNlRRUFpA3NE4UvJSKgOtMgsymdhtIl5OXny38zs+2vARiTmJJOclVx6XOSmThs4NmbFjBpsOb6Kx\ne2Pis+KJPRJLiaWkMoXoxlk3smjfIgI8Aii2FJOen073gO6svVeqyHX4rAM7Unfg4ehBu0btGBk+\nktvCb6OJe5OL8n0YLl3MHLjBUANZWWIIl3uvufFGSW/evFlqR0vdEk1hoQJC+ct2D4MHBrG/w128\n+UNrGjeGwEA32g90Y0gAuIcHgi0cyGrAr7Nh2lcAHQgO7kDfvvD5dDGgn34avo0QK9nf34nHH4fb\nR8iYPD3h7bdliYuToLG1a8X4Bjl28WLJa66ITevUSTo+AWzdKq7vyZOl8EinTvUrNarMWoadjR1Z\nRVnMiZ5DaINQmnk1I8AjAFubk/R8rIESSwmJOYkczDpI3NE4Dhw9wIGsA7x67au0aNiCGTtm8NBf\nD51w3G3ht+Hl5IWDrQMNnRsS4RdBWMOqqlsejuKOGNd+HOPajzvp9Sd0nUD3pt3Zf3Q/LvYuNHJt\nRJh3WOX2RXcuwt3RHRd7E0JvOHeMBW647LBYZJ43I+PYAmK9e8tcbkyM1Ik+evTY4774QnoH79mS\nz6RHC/EO88G7ocZn3jd4t/Lhhpe6EdTFj9LS09d/rugItWqVTFMfOCAPBkrByPIYoLvvlsyts6lv\nMmmSWObVm0q0aCHR3PUJq7YyN3ousZmxxGTGEJsZS2xmLPd2vJd3rn+H/JJ83N6qShWyt7EnyDOI\nyX0nc3eHu8kryePH3T+Snp8ukdT5Ek39cr+Xua7ZdSzYu4DB3w+uPN7Oxo4gzyBmDp9Jz8CexB2N\nY+PhjTRxb1JZdKShc8PzLr9pMNQ2xoVuuKyJjhbXcceOklOcnS2xXsfzvNdU3gj7lhzvZjwb/yCh\ng1sT2qMRzTwyaWbZh5evvXS3mj5dFDgpqU4mgM8ic6tesyp+FTEZMRzIOiBWcNYBOvl34tObPgXA\n+z/eHCk8QhP3JmLhNgxjSNgQhrYaCsCBo3JcxbFxR+MY224sQ8KGsDlpM12/7AqAh6MH/m7++Lv5\n82LfFxkQOoDk3GQW7FtAiFcIoQ1CCfAIwM7GOBwN9Q8j4IbLksOHpUzm9OkSlPXBB/B//wfaqpn1\n6n58/Gzw7hqKd9Z+vEdch8d1XVD5eTIXnZoqeU09eohojyt3h9rZiYn86KNirhtOidaaxJxENiVt\nYtPhTVi0hf9cLyluFfO8djZ2BHsG06xBM/o368+zvaXdZ0xGDE3cm+Du6H7W1y0uKyYlLwVfV1/j\nhjZc1hgBN1x2fPmldJMsK5O+xZMng09mjNTm/v578aGPHi3vQcqGOZzEPZqSIrnUR45Ig4wmJpDo\nZOQW51YK7kt/v8TnWz4nLT8NEDd1n6A+LL9rOQC703bj7uBOU4+mxvo1GM4RE8RmqP+88grFO/ZQ\nUlCGe0kmLZKvYpjXSF5ffQ2hoVTlSysl9TdfeEGSmys4mXiDhGoPGlTnt1DfsGorezL2sC5hHWsT\n1rIucR17j+zl6DNHcXNwo5FrI25scSNdm3SlS5MutPdvX5k2BdDGr80pzm4wGM4XI+CGS4KMDCnN\nrVRVi8XVPyeRbteYgkJFxtqufPD3vQxrsIr3wz7l2qD9XNt0BoReIzvfeKMkLo8cecVa0FZtZVvy\nNorKiugVJFFtFqvljKK3s4uyKwPKBoQOwN/Nn6mbpjJxwUQAGjo3pEdAD+5oewdlVunf/cjVj9Td\nzRgMhtNiXOiG80NrSUg+h44SM2dKvvPatZLPDOUpUTOj4M036TzrCbbSuXL/Dh2kguj119fW4Os/\naflpLN6/mIX7FrJ4/2LSC9J5ttezvDXgLbKLsvH5r0/l/HOoVyjNGjRjUItBdPDvwM7UnUyYP4GY\nzJhKNzhQ2f94/5H9rIpfRc/AnoR5h9W7JhAGw+WAcaEb6obSUnjoIdi7V3pE5+dLT8eRI6VmZ3nN\nzcJCaTO9dq2U9v79d7G0Fy+WgiO9ekHXruCdfwj/xTOgzUvg4sK3d3ek7I5gXIJ8cHERw7o+5TTX\nBSWWEg5lH6JFwxZorenyRRcSchLwdfFlYPOBDGoxiEEtZDrAoi083fPpyijuOXvmkFGQgYejBx38\nO1R2dLo57GbCvMNo5d2KMO8wmjdsDkDzhs0r3xsMhksPY4Ebzo3cXBHpRYvgpZckHHzHDmlJFR0t\n0dyDBpE65B4Gf34LW7fb4OAgFvbCn7LxzE2keH8ijsU5cp6yMukRmZ0tEeCPP26KbgP/xP/D3wf/\nZnfabiLTI4nNjMXP1Y/EJxJRSjEvdh6N3RrTsXFHbNTpn25yi3NRSp1zO0aDwXBhOW8LXCk1CPgQ\nsAWmaa3fPm77g8AjgAXIAx7QWkcppUKAaKCipc56rfWD5cd0Br4BnIH5wGO6Pj1NXMkkJcGQIbBr\nF0ybJr2lQXzckZEi5LNmkfbdYnrOa0eKM/z0E9yy/jkcp02BYKkb7giSZz1ihAj+7NnQsmXNSdyX\nMfkl+WxN3srmpM1sTt5MVHoUG+7bgIOtA79G/crHGz+mWYNmRPhGMDRsKF2adMGqrdgqW24Ku+ms\nrnUuKVsGg+HS5LQWuFLKFogFrgcSgU3AaK11VLV9PLTWOeXvhwIPa60HlQv4PK31CeGoSqmNwKPA\nBkTAP9JaLzjVWIwFfgmgtQSLbdsm/SVPEb1tLbXwxJ3pjHnSX1Kqv/tOum9VlBCrKCcWGHhlVDZB\n8pd3pO6gtU9r3B3d+Xzz5zw8/2Gs2gpAgEcA7Ru15+tbvsbX1ZfMgkyc7Z1NrrPBcIVyvhb41cA+\nrXVc+cl+BG4BKgW8QrzLcQVO+VSglGoMeGit15d/ngEMA04p4IZLAKXgs8+kg0anTjXusmQJNG8O\noaG2fPhTtQ4cd94py2VIUVkRW5O3sidjDxkFGQxuOZg2fm3YlryNRxc+ekzDDIu2sOCOBQxqMYiu\nTbsyuc9kujaVVCx/t2M7lni7mGkEg8FQM2ci4E2BhGqfE4ETSlQppR4BngQcgOuqbWqmlNoG5ACT\ntdb/lJ8z8bhzNq3p4kqpB4AHAIKCgs5guIY64YcfYMMGeP99CA8/6W4zZ8I990iLyl9+uYDju8BU\nNN04mHWQ0bNHszV5KyWWksrt3s7etPFrg4OtA3Y2doT7huPt7I2fqx8d/DvQtYmUAe3UuBOdGtf8\nIGQwGAynotai0LXWU4ApSqkxwGTgLiAZCNJaZ5bPef+mlIo4y/N+AXwB4kKvrfEazpCyMsndeuEF\n6NtXLG9n5xN20xr++1945hno3x+++uoijLWOKLWUsj1lO+sS17EuUYqajG4zmrcHvE0j10Y42Drw\neLfH6RHYgw7+HfB1qSrvGUS6fmgAACAASURBVOEXwd93/X2R78BgMFyOnImAHwYCq30OKF93Mn4E\nPgXQWhcDxeXvtyil9gNh5ccHnMU5DReQ/Hw4GFNM3OdLuHHhY9gdimN9/xfYMexl/Bfb4+8vxcsa\nNZL2mFar9Jv+8EOpXvrNN6cufHapk5qXSnJeMh38O6C1JuTDEJJykwCZo+4Z2LPSgna2d2bl3Ssv\n5nANBsMVypkI+CagpVKqGSKyo4Ax1XdQSrXUWu8t/zgE2Fu+3hc4orW2KKVCgZZAnNb6iFIqRynV\nHQliGwd8XCt3ZDhjUlOhQQMR299/lx7UBw5oUlMVEiN+Ewc6fU3Ix+/z25qbeWfiiYFmeXkyLb5m\nDTzxBPzvf5dmrnZBaQEL9i4gpziHgtICoKqS2Fdbv2LD4Q1kF2ezJWkL+4/uJ9w3nMiHI1FK8VLf\nl6QSWWAPAjwCTnUZg8FguGCcVsC11mVKqQnAIiSNbLrWOlIp9SqwWWv9BzBBKTUAKAWOIu5zgL7A\nq0qpUsAKPKi1PlK+7WGq0sgWYALY6pSyMsn6WrdOCqqsWwdxcdKvuk8fsMvKwOXwUW6yRBL6+lCa\nhdoQ6pZG44GzwRFeuxEefUz6flQs6eng6irnX7nynIqxXRByinMYPGswaxLWVK5zc3CrFPD1iev5\na+9fuDq40q5ROx7s8iA9A3tW7ju+y/gLPmaDwWA4HaaQy2WM1lIszcFBRLuXlMemcWPo2RN6dCpm\nZNM1BK6YKV28rFYYNQqmTAFPz4s7+Fpk4b6FDPtxGF/c/AV9g/viYu+Cs52zyYk2GAyXPKaU6hVI\naircf7/MVX/xhWR8ff95Dj1ZR1DfENRVrWDxSrjhBglKGz8ennoKQkIu9tBrDau2YqNsGNRiEHGP\nxdHE/cpscmIwGC5PLsHZSsP58ttv0Lat1BoPd09AP/QwTl3aMHq8J8HjB6F+/EF27N0bli2TVmAf\nf3xZiXdafhpXf3k182LnARjxNhgMlx3GAr+MyMmBxx6TKPCOHWHm5wVE3NgRiotFrO+4Q1LBupR7\nY1xcpHf2ZUZybjL9Z/TnYNZBnO1OTHkzGAyGywEj4JcRmZlifb/wgvQXcXBwkfriERHg43Oxh3dO\naK1ZfWg1SblJ/KvNv4Aq13hNJGQncN2M60jJS2HhnQvpG9z3Qg7XYDAYLhjGhV7PKSqSoilaQ7Nm\nELfPyuv5T+Dw1aeyQ79+9VK8rdrKHzF/0Gt6L/p+05dXV72K1hqL1UKLj1pw60+3Mn3bdFLzUiuP\nySjIoN83/aRH9p2LjXgbDIbLGmOB10Pi4yX9a+VKqTt+6BC0agW9OxfSYPxYsbqffPJiD/OcWRa3\njEcXPkpUehQhXiF8cuMn/F/H/0MpRV5xHoNaDOLP2D+Zu2cuAFc3vZpXr3mVgc0HMjJiJLe1vo2u\nTbte5LswGAyGusUI+CWO1rB3r3TbDA2VJmAVPUS8vGRq+/PPoXfrTBgwVBK833tPqqpcABKyE/hi\nyxc42DpwV4e7CPI8t3r1+SX5FJUV4e3ijYOtA7bKllm3zmJkxEjsbKr+mXo6eTJ1yFSmDJ7CjtQd\nzIudx5+xf2JrY4tSircHvH2KqxgMBsPlg8kDv4RZvhzuugsSE2HiRPjoIynI8tlnEovWpk151bPC\nQolaO3hQWnbefvsFGd+Ti57k440fY9VWKv4dTeo16YxFNKc4hyX7l/Bn7J/8EfMHo9uMZsqQKZXn\nUldIi1GDwWA4GSYPvB4ycybcey+EhUlAWkWwuJ0dTJhw3M7OzvDww9C5c1W1ljrAqq0s2reIAaED\nsLe1J8QrhAldJ/BY98cAmL5temVnrbT8NN5d+y73drqXMO+wE85155w7+TnyZ0qtpTRwasDgloO5\ns520GjXCbTAYDKfHWOCXIL/9BsOHi2jPmXOSomhHjsCrr8pOQ4fW6XgKSwuZsWMG769/n5jMGH6+\n/WdGRIw45TFzo+cy4pcRWLSFfsH96Ny4M7vTd7PwjoVSX/zvlygsLeTmVjfTM7DnMW5yg8FgMAjG\nAq9nDBoEb74phdFO6OpVUgJTp4p4Z2eDt3edCXhhaSGPLXyM2dGzOVJ4hM6NO/P9rd8z7Kphpz12\neOvhJDyRwDfbv2HatmmsSVhD3+C+ZBVl0cC5Aa9e+2qdjNlgMBiuFIwFfomQlwfPPguvvSYdwmpk\n4UKZDN+3T0qg/u9/MhFeS1isFlbFryIhJ4Fx7cehtabLl11o5d2K8Z3H0ze47zm5t63aSqmlFEc7\nx1obq8FgMFwJGAv8EiclBYYMge3b4frr4ZZbjttBa+nZmZoKjo4i5DfcUCvX1lrzz6F/+Gn3T8yO\nnk1qfioBHgHc2e5ObJQNm+/ffN5z0jbKxoi3wWAw1DKmkMtFZs8e6NFDXv/44zjx3rcPxo6FDz+U\nz2PHisrXkngDvLbqNfp904+vt39N3+C+/DLiF2ImxFRWOjMBZQaD4UqgpERaJNcnjAV+EdmwAW68\nEeztpShLRYlyoqJkEvyHH2QSPDxc1tvYlOeN1R73d7ofV3tXHuzyIK4OrrV6boPBYLgQJCfDQw+B\nxQIDBsgSHi6Oy1Nx9CjMny/G04IFkJsr3ZQffvjCjPt8MQJ+EWnUCPr0gQ8+kDKoALz+uuSNubhI\nNbWnnpKeoLVIXkkeH67/kGd6P0Nj98Y81fOpWj2/wWAwXCh27ICbb5bEnCZNYJ40IMTfv0rM+/eH\ngABZf+CACPbvv0tFS4tFfotHjZKaG488IjFJkyZdvHs6U4yAXwQKC2UqOyRE/hGxcSM4BULjxqLo\nzz8Pjz9eJzXMjxYeZfD3g9l4eCN9gvuYeuEGg+GUWCxSrrnSyLiEmD8f/vUvSbVdvRo6dJBS08uW\nwdKlsGiR1LYCKTdtbw+7d8vniAgR6Vtuga5dxblZWgrjxsEzz4g1/uqrp7fiK0jOTWb1odW4Orgy\nuOXgurnh4zBR6BcYiwWGDQN3d5j1QhTqySekcfekSfDOO3V67ZS8FAbOHEhMZgw/3vYjw1sPr9Pr\nGQyG+k1qqnQhXrZMMmReeOHMBa2u+fhjsXM6dIA//xTr+3isVhHspUtlKS6Gm26SzNvmzWs+r8UC\n48dLk6jHH5fK1Mffs9aamMwYVh9aXbnsP7ofSpzp5z+MFRO/r7X7NFHolxDPPScunqmPxaB69ZRH\nwnfekQmcOuRg1kEGzBhASl4Kf435iwGhA+r0egaDoX6zciWMHi3zxP37w4svQlqaTPmdEIpz4IC4\nFps0EXO4DlW+rExaPXzyiVjPs2aBqyuizra2Uq6yHBsbaNdOllP1d9Jas2jfQubu/hXsbHEc7kDH\nzBF88EE//o7ZxM1P/oWjgx1aa7ambGX1odVkFGQA4OviSxePIYTGf8fG3zrj1c8WJtbZ7R+DEfAL\nyNdfw3//CxNuOshDU9tKndT58yHo3BqAnA1p+WmUWEpYOm4p3QO61/n1DAZD/cRqhbffFsFu0UKy\nVtu0gaefFms0PR2+/baqyFTuTzPZ89Q4Qo6CbwHg5CRCfvzSvj1ce60YLWdAfkk+O1J3sCVpC1tT\nthKTEYO/Q0t2T3mBvRvCuPuhTKZ+4I5zcTG88p7UxSguFtP6qqvEZ96qVdX7hg3LT5wvHaJiY7Hu\nieb3pOW84bKZLZ4FeBSBcxmU2ilKwj/FLvP/sWPBi+xI2AfDx4FtGc3dgxnScgh9gvrQpPg65kwP\nYeZMRUmJzMU/dQEbQRoX+gXin3/kKfbaa+Gv6anYPT9J0sO8vOr0usm5yTR2bwxAcVmxycc2GK4A\nLBbpbRQVJSmqQUFS4bHGsszVyMiQbNWFCyWo64svZLoPpBzFS6/n8vpL7rTsGkebiS+zK/lv9hUe\nrjy+pfKhd0kjemW60euQlVb7slCHk0Q0QX7vhg6FW2+FgQPB2RmL1UJafhr7j+5nS9IWtiTLsidj\nD1ZtBcDP1Y9mqh/b3nuFkuSWMOQR6PIFdtjQ8ogiPMVCeMNWdGwYTo+9RfhHxotIl5ZW3ZyPjwQf\nHT5MmQ38HAFv9oFIP2he6Mxzpd0Y2/BaHPKLxO1Qvvwn6iaeSZzITQ7zmWm5FU9nB1ZN/IV3dw/k\nzz8VTk7SdOqJJ+Q5obY5lQvdCPgFYtXfZTx/Xxrz1vvi5XtmT6Dng9aaaVunMXHBRKYOmco9He+p\n82saDIYLi9awfz/s2iViXbHs2QNFRcfua28P11wj+jl06ImOvzVrRLTT0sS2GD8eUvNTmB01m/n7\n5rM1eSspeSmw7W74YxqOATsZ0G0w3aylRDzzLvtL01idsJo1h9aQWZgJgLezD20sd6C3jaQw3oGy\nvERKiw9T5JBLgVsBuR4l5Dllo+3zwaYMij1woymN7MJoqEJw001wKPOhpMCJnTsVpaXw3awiAtPe\nI2rWB0SRTmRrH6KCXdhXmFgp+CFeIXRv0o0ezi3pnt+ADollOMTso6S4gJkt8nnbbj37SlII927N\nC/0mn9C2+HimTpXo9D5diyiMPsDmvNb4OOTwyOP2PPyUM35+tfc3PR4j4BcRiwVsC/Ng1Cj0X3+h\n5syRTiV1SH5JPg/Pf5gZO2YwsPlAZt06Cx+X2o9oNxgMZ4/WMmW8ZmUZ676Pg+ISut/sS/dbGtGy\n5ZnlLldEWC9eDAkJVduCgiT/ufrSqhVER0vGy++/Q2ys7NuxIwy9WTM05zuWr3Xk2S0jCA5WfPpt\nBnsdfuKXqF9YFb8KjSbMO4zuAd3p0KgDHRt3JGlJc+4d70uwTQKLVzgS1LvqaUBrzcqdcXw87QjL\nf29CVkJTsCkBr4NQ6oIqc4USF7Tl5N5AGxuNp5sFD08bPBvY4OEBfn6aV69fTcRnEyV3rH178fXf\ncAMoRVFZEduSt7EucR3rE9ezLnEdiTmJADjaOtK5SWcSshNIyEmgU+NOTO4zmVuuuqWyaNXpmDED\n7rkHQkM1T0UsYty8kTj7uct8woC6iykyAn6RKCuDWwYV0zv6S55LeUwe48aPr9NrxmTEcPsvtxOZ\nFsnL/V5mct/J2NrY1uk1DYYrhRJLCbnFueQU55BbkkuppZQQrxC8XbxPekxxMWzdCmvXipW7dq0m\nNVVU2p0cAHLxAKChYx7d2+TR48YG9LjGka5dpSTExo1Vgr1xo8xTu7qX4h2xndyAOdx6XTBvjbob\n3wZOp72HmJjyPOjfNGvXanR5Qc4Ofr/j+vh/WVuyFo0m3DecEeEjGBE+ggi/iKoT5ORA3778E9uI\nm23/ws3TjkWLIDAQZs+WVsgrVsiDSq9e4pK/dnAGTh4F+Lv542Ark+dlZVCQayF/5WYKfl9C2aZt\neByOxiMrHhcKqHyO8fKC0FB5stmyRd6/9pq4C05T2CoxJ5H1iesrBd3ZzpmnejzFoBaDzqnKZFqa\neOJtbJA/6h13iLvj8cfhrbdk/r+WMQJ+ESgshAljs5g+24vP7Ccyfs4Nkr9Qx8yNnssD8x5g1q2z\nGNh8YJ1fz2C43LBqK6viV/Hdzu/YnLS5Uqxzi3MpthTXeIy3szdh3mGVSyPdjv2rO/HPwkZs2mhL\ncflhzX2y6VmwlJ4FS+jVJofw18dAWBjRMzez/q9M1kV5sL6sC1GIYCqlcXWykFdoh42Npm37Ajza\nbCS2wcekev6Bh4srnRp3YsXBFbRs2JLPb/qca5tde/qbzM+Hf/2LxZs38ky3/2O7w0GI+JnwdBjh\n1ZMRD08hommHE48rKZHfseXL4a+/2Nn4BgYNksInpaXitm/RQkT7zjtFa8+a7GxxUcTFyVLxPiND\nJpsfeKCGNo0XiYICSRr/5BOJ9Js1S0LeaxEj4BeYDRvg7rvlwez5hp/yxsIuUimgjiixlLAuYR39\nQvoBkFOcg4ejR51dz2CoazYe3ohVW+s0Y0Jrqd6VkCDL5uhklmyPYUfsEQoyG6BygnB2cKJx+D6C\n2sXTsmMSgcEWPJzc8XD0wN3BHVsbWw4cPUBMZgxRCYfZtbIlWZsGwYH+oG3Bbxeq5WJsA9Zh778a\nO5dUbGxssXV2wdbBCVsbWxo4NSDYK5gQzxCC3ZoSklqM94Z0MhZYidnTmESbRtgFrWBHz6WsDzuK\n0jDwoC13HfBk2FE/nL18WTKmGw+VzGX/0f3c1f4u/jfwfyefNktLY8uYa3nFL4o/W0EDpwY83PVh\nRje/hYi3voLPPxf/+qxZ0Lr1sV/Y3XeLL/nrr+U9oq/33y+u+rFjoVu3SydX/IKxYIH4148cEUv8\nVDlrZ4kR8AtIZiYEBWm8vRVffQXXX2eR3MQ6IjEnkZG/jGRz0mb2TtxLsFdwnV3LYLgQzImew6hf\nR6GUOqeaBUVFEti1f78UIklPlyUtrep9xVJSctzBNqU4NTxCcKCibcuGlBbb8c8/8rsMUo6zb1/o\n20fTz2kDQau+46/4CL6P68H8Q20osdrRzCuDa8M30Lr9MooabqZg5xYshQVYggOwdO+GtWkTLNqK\nxWrBoi0cKTzCwayDHMw6WBn8VYGjrSNYrRTrUlrZ+XO3TSfuLAojIAdxZWdni098924K27Ti9Yci\n+E/mH3g6evLeDe8xtt3YY1zFWzb8xitf3sGfgQU0sHXjqb7PMrHbxGMf+H//He69V6zL996TaT+l\npIrLm29KebIXXzyrv8kVQXq6eAdCQ+Hdd2vttEbALwAHD0ppVOLiWHD1y/Sc1BvPSXU3311YWsjH\nGz/mrdVvUWYtY/rQ6YyIGFFn1zMYLgSzds7irt/u4uqmV5Nfms/+I/trrF1QVASHYgrZm+hMbKxk\nDFUshw6JsVgdNzfw9QU/P3DzKiTf4SAp1l3EW9ahPQ7RKtSV0b16c1+foTT1PLb3gNUqQWCrVsHK\nlZpVS4pJPiJznTZYsGKLv00q/7Kbw2ibn7jasg5lKZMDQXoEv/QS9O592vvPK8kjPiu+UtDjs+Mp\ns5YxMmIk3Zp2q3neVmuYOxcmT4boaHZfE84DN2nW5UVzXbPr+GzIZ+SW5PLKb4/zR9o/eBUpnmpz\nPxNv+w+eTifJK0tOFgt78WJJbu7VC559Vkztzz+/Ak3sM0RriVy2q70SK0bA65CSEnjjDXkwnf1p\nGkNfv1qK6P79d63PhVRe01JCxNQI9h3Zx5CWQ3jvhvcI8w6rk2sZDBeKL7d8yfh54+kXfA1fXvsn\nu/cU8eD3b5Cd5sGwxhMpPOIt7u5DmvSMYwXEy6mQlkHFtIxwJKy9My1bylxs48bg7WMl6uhW/oz5\nkz9i/2B7ynYAWrgFc3urYYzt9gDhvuGnHpzWUkLxtdfQmzaxv3EfVvV/hdhGfRg42I5+/WpwtFmt\n8mN+hoVLzhuLRdzeL7+MNf4gX/yrBc9EJFOoSyi1luJVBE9FezHx9aV4tul8+vNZrVKvdNIk+aEb\nMgR++61Wxclwes5bwJVSg4APAVtgmtb67eO2Pwg8AliAPOABrXWUUup64G3AASgBntZaLy8/ZgXQ\nGCgsP81ArXXaqcZxqQn4jh0SU7FjB4y7vYAPNvWiQdYBCfDo1KlWr6W1Zl3iOnoG9gTg002fEu4b\nXjnvbTDURwoLJW/53blL+GHZbrxzrkWltSfjOIFWTtm0bOZE88aawJ3zCcjYTtB1LWhpG0dY3EK8\n929EUf5b1qQJ1s6dWNzOld90NH/a7CXJrhAbDT1THBgaWcbNe6y0ykAinYOD5f9rxdKxoyg/iIjN\nnStdArdvl44ezz8vHS8ulUCq4ykpgS+/hNdeIzk/lbdGNsZvfwoTi9vj+dtCab11NuzcCb/+KsFa\nrqbl8IXmvARcKWULxALXA4nAJmC01jqq2j4eWuuc8vdDgYe11oOUUh2BVK11klKqDbBIa920fL8V\nwL+11mesyJeSgCcmSiVUDw/44pMShr7YXlYuXSpRHLXIhsQNTFo6iVXxq1g+bvmZRZkaDJcQR4/K\nVG1srCwxMVJ8ZO/eKk+zrWMRndo50L69De3aSVBUYCDkO8Uw6JdeuCtHVk/TNE3Mhu+/l0LYFeTm\nwrZtsHUrayMX8oTzSjZ6F+FWDIOSXbg525/BugU+DQPEj96okeQDJSVVHleZIA3Si7JTJ2ltFRkJ\nLVvKHPCYMRfOoj5f8vMlOvqdd6BnT/jxR5lLMNQrzreZydXAPq11XPnJfgRuASoFvEK8y3EFeRTW\nWm+rtj4ScFZKOWqta87FqEds2SL/j9euhdBQB8h6Sn5xakm8LVYLi/Yv4qttXzEneg5+rn5MGTyF\n3kGnn0czGM6VnOIcxs8bT0J2AhG+EbTxa0OEXwQRvhH4WZ1RO3aI2BUVSfGKjh1PyMU9cgSmTxfL\nukK0MzKqttvaiiHbpo3G9+q/WV08hZv7hPDL+HdwtK8pr7cVC4Oe47qd/+b6G+1YNWoRPt2vO3YX\nd3cOtgvimbQp/Jy9iCbuTfi636uMbn/nmZcPzskRd9rWrSLqW7ZI6c3vv4eRI+s0GLVOcHUVq/mp\np2TsZt76suNMLPDbgUFa6/vKP48FummtJxy33yPAk4i7/Dqt9d4azvOg1npA+ecVgDfidp8NvK5P\nM5hLyQIHKEg8gktCDPToUSvnyy3OZd+RfXRs3BGrttL0vaYUlBbwZPcnearnU7g5mKdnQ92Rlp/G\njbNuZGfqTro16kxUehRHy3Irt3sXQEQaRKRDp2QYtA8CHH2lpvWgQTBwIPM3+3HffRID1bixeKla\ntZLXiiU0FOzsNE8seoIPN3zI/Z3u59Mhn9ZccEhriYR++mlWXh/GoD7xtGnUhmXjllVGTucU5/Dm\nP2/ywfoPsFE2PN3zaSb1moSrg3H3Guo/5+tCPyMBr7b/GOAGrfVd1dZFAH8g89z7y9c11VofVkq5\nIwL+ndZ6Rg3newB4ACAoKKhzfHz8aW+4rklOBn+fMmkHum+fJEKerkvAScgoyOCPmD+Yu2cuS/Yv\nwcfFh0NPHMJG2RCVHkXzBs1NAxJDraC1ZB2lp4th6e4ui13OEeLXzGfgtidIsBzl12XeDF6ThgZS\n3CCyjR+RbRsR2dSBSNcCIksPk10iTre2xV4M3l1M3+12zE54l+ncTxvfFGa8lUTHsW2OmScus5ax\n78g+dqXuYnb0bH6K/InHuj3G+ze8X3N0dWkpTJggHTVuuw1mzGBe4nKG/zScXoG9mDdmHrN2zuLF\nv18kvSCdse3G8mb/NwnwCLhA36jBUPecr4D3AP6f1vqG8s/PAWit3zrJ/jbAUa21Z/nnAGA58H9a\n6zUnOeZuoMvJHgoquBQs8NJSsSBubhfP1PkhUgd33LhzOtd7697j6SVPY9VWgj2DGX7VcG5tfSu9\ngnqdcX1ew5WB1iK8+/ZJrQGL5eRLUZHkP6ekVL1WLMU1TF45UUCJSy445tC8uIgmrrZ0apXP6FGa\nLv9qjvLxPm4smqj0KBbsW8D8vfNZucIG69xpkBNIqw4f86R+ln4JxRxs5MiurkHsau7GLtd8ogri\nKyuZ2Spbnu/zPK9c80rN4p2WJs2oly+XoLHXXqt01f+w6wfumHMHbg5u5Jbk0juoN+/f8D5dmtT4\nG2cw1GvOdw58E9BSKdUMOAyMAsYcd4GW1VzmQ4C95eu9gL+AZ6uLt1LKDvDSWmcopeyBm4ClZ3db\nF4eff5ZYtSHNfpQgmFGjzvlc14dez30d72N8l/F09O94TrV5DZcXR47I3PG+fbLs3Vv1Pifn9MdX\noBT4+moaeRThb59BmN0h/F334F8chQ/plOBAbsMQogMDmOmahWOpF9c0GowtLTl6FKZshPdXQ4sP\nJG5rzJiqVolKKSL8ImjmFkHiz//m72+gcXAeXZ7/D1vsPmF8bsVTQjGwl8bZ0DYWJhy1p613O9qG\nX0vrfrfhrBzgl18kcTs+XpaK91lZEmRSwwPy6LajyS/N59PNn/Jc7+e4rfVt5v+O4YrkTNPIBgMf\nIGlk07XWbyilXgU2a63/UEp9CAwASoGjwAStdaRSajLwHOWCXs5AIB9YBdiXn3Mp8KTW2nKqcVxs\nC1xrCUwtLrSw+4AbNuPvh48+OuvzRKZFEu4bbn50DIBYzYsWwbRp8Oef0uQBJO4oJITKnOYWLeS9\nn59sq3HZH4vDr9/ju2s5dts2VfWU9PGRAMtu3eDqq6FLF5Zlb+eWH2/Bz9WPJWOX0Lxh88oxZWVJ\nY4rvv5eSBhX/9seMkWfWhARJoYyNhYkTpXqkq6tY57vSdrE5aTOhDUJp49cGnzwrrFwpJ/r7b6kx\nfDyenpLOFRQkr8HBUgClQw31uA2GKwhTyKWWWLZMAm+nPbuXez/vJr+6Z1njfMHeBQz9cSgfDfqI\nh7o+VEcjNVxyFBWJeZ2fLyLl6Eh8vERrT58uXh1fXxHF664ToQ4OPouMpdRUqfY1bZoc1LlzlVh3\n6yZPAtUeGGdHzWbMnDGEeYex6E6J2j4ZSUnw008i5ps3y2mUkrKiX38t4z0rkpOlLZeTU5Von2MM\nicFwuWMEvJYYNUra5B08CE6qWAJ0zsKKXpuwlgEzBtDatzV/3/W3aThyOVBaKkW3o6NlOXhQhDoz\nU14rloICAEqw5w81jGlOE1hcKCmBAyOSuH90Hjff44tD45rbUsZkxDB3z1x+2/MbRWVFDAgdwMDm\nA+nj0xnnjz+VXN+iInj4YalT7VNzI4uisiK+3vY1ExZMoFvTbvw15i8aODc449uNjYUffpC59Gee\nMbprMNQ1RsBriaIiiN5RQser7c86p3JX6i76ftMXXxdfVt+zGj9XvzoapaHOOHxYXMHR0TJRHR0t\nk9OlpVX7VBQIadiwcjlsH8KyIx1YmtCKhVFBpOc6E+CcwT3uv3JP1nsEl1SbYfLxga5d0b17s7VD\nI+ba72Xu3j+JSpeyC12bdMXd0Z3Vh1ZTYinBsQz6xsP1zhEMvOdN2l19c+XUTHp+OjtSd7A9ZXvl\nsidjDxZtYVCLQfw64leTamUwXOIYAa8FtC7X7FdflXrD27aBi8sZHVtQWkCrT1ph1VbW3LOGEK+Q\nOh2roQ7YuRP69ZPJLVxt1wAAIABJREFUYVtbaN5cWi2Gh8tr69Zw1VXg5kZ2tuj80qWwZEnVlK+v\nL/TvLy0Xb7ihvC6IxSJBW3v2UBi9i3VxK/kjawNz/Y5wyAtsrdA325PhHt0Y1nUsgdfcAlu2UDDp\nCVYd3c7ino1Y3NaFyPwDADRybUQbvzZEZ0STlJtUOfwAjwA6+HegQ6MOdGrciZvCbsLetp5UFDMY\nrmCMgJ8nSUnyw/v5Z5q+97aUObvly8/qHHOj5xLmHUaEX0QdjdJQZ+zfL52kbG2lLna7dpJIXU5+\nPqxbJ9Mry5fDxo2iyy4u0npywABZ2rY9tmhZVlEWaxPWsip+Ff8c+odNhzdRai3F0daRgYHXMNzS\nkpujLPis3CQPjBaLnMBqlX+Db70l8zo2NhzOOczSuKUsjltMTEYMrX1b06FRBzr4d6C9f/uT94Y2\nGAyXNEbAz5PnnoP//Af2/rSV0BGdj2lmfyqyirLYmryV65qdbZSP4UITGyulop2dpf52UJAsjUnG\ntl9vsbz/+QfCwykokBK6K1bIsnGjeNFtbaFLFwmeHjAAunc/RucpLitmXuw8VsavZFX8Knam7kSj\nsbOxo0uTLvQN6kuf4D5cE3LNiVX3cnNh/XoZQ8OG0qPZ2flCfkUGg+EiYAT8PMjNlR/yAQPgF5+H\nJC81NVVKWJ2CUkspA78byMbDGznw2AEz530JYrHAggXS72HRopr3saWMpiqJoHYNCAx359ChEwX7\nmmtk6dWr5n8W+SX5fL7lc/639n8k5yXjYu9Cj4Ae9AnqQ9/gvnQL6IaL/ZlNxxgMhiuL8y3kckUz\nfboYX/9+tARu+QluvfW04g3wzNJnWHFwBTOGzTDifYlx5Ig4UaZOhbg4aNJEQhvuv1+M2oQEOLAn\nnz0vfMjeJMWBTrfw/9u77/ioqvSP458nCb0KhCJYEFE6QQNrRYoiRREUFUQRUVhU1nV1EdRdC+vu\nitgVLOu66G8V0CAKiAWQsipoQINUBQExlBQ6aaSc3x93iENIyAQSJsN836/XvDJz6zNXw5Nz7rnn\n2ZZembWL8qhTN5c7/1CFXldUKTJhH7I7Yzcvf/syL3zzAjszdtLlzC7855r/0K1pN91/FpHjphb4\nUeTkeJNnnHYa/O+LbG+WjTPO8J6xPYqpq6YyaPog/tDpD7zYq+QTvUjJbd/ulWuuVMl7vPjQT//3\nv/7qJe133vHqUF90STZXDtpAnQ6LWbvrB9akrmHHgR2kpKWwK30nrogHDQyjXYN2dD2zK12bdqXz\nGZ2pXbl2/vqkA0k8t/Q5JsVPYv/B/fRp3oeHLn0ov5a7iEig1IV+jPLyvJx9yineYKRAbNq9iTav\ntOG8Rucxf8h8KkZWLH4nOS6zZ8NNN+ewf2/xHUpRlQ7S4MJ5HDz/OVJq/DZ7b61KtWgV3YrGNU4l\n+usVRK/YQP1+NxF9+TVEV40mulo09arWY+PujXyx6QsWbF7A179+TWZOJhEWQYeGHeh6ZlcycjL4\n9/f/Jisni+tbX8+DlzxITEPNJiYix0YJ/HilpnpNtxEjoGHDo27qnOOlb1/i+lbX06hGoxMUYHjK\nzYXHHoMnngBrlIC74j6IyIWcSpBT2XvlHnpfCaIyqXTOdFpHQZsaZ9OmaSfatO9Bmybn0aRmEwzg\n7rvhFd/EKA88cNTzZ+Zk8k3iN/kJfWniUhyOW9rdwthLxnJO3XNOxGUQkZOYEvgx+OYb+PhjGD0a\narz1sjfh8w8/eM8CFSLP5bF9/3Ya12x8QuILd6mpMHgwfP45WIfJxAz7FzOHTKNGxRrk5OWQu/VX\nct58g5x3/0vO/r3ktmxB5JU9OWPtNiKXfuvNmAbetKMdOng13TMyvNKVDzzgJfASSs9OJzMnkzpV\n6pTulxWRsKVBbMdg0SKvguGf/gS8/Ta0b19k8gZ4YvETPLf0ORJ+n8AZtc84cYGGofh4GDAAtm3P\nxa6+i4v6reHjm+ZQq1JN74HsF17wKnE4B9dcA3/8o3cPxH/2vB07vMeyli719nn9dS+B33EHPPnk\nMcVVtUJVjSYXkRNGCbwIa9d6veWnJK3zMsYzzxS57SfrP+GxhY8xuN1gTq91+gmMMkwsWQITJuBS\nUvlXan/+8NMoqlVNIWdoX3rU2cGM5b2puupx7xnpZcu8Cbr/+EcYNQqaNi38mA0bQr9+3gu858K2\nbfOeGVSVOBEJAUrgRVi3zpsdk//7P2/2q5tuKnS7jbs3ctMHN9GuQTteu+o1lQgtTfHx8Oij8Mkn\nZNRtwl0VXmfyjl6c3XABG4YM4NqUTN59pxaV0qd7E9WfcYb3UPett0L16sUf31+FCt7+IiIhQgm8\nEM55LfDBg4Hdu+GqqwodvJaenc61064FYPoN09V9WowDB7xHuX791Suf6f9++3bvsT3nwGVk4lJS\ncWk1cREv4urWYa/VJmVHBBfePJclZ/XklpjBvHnNm0RF6H9hEQlP+tevELt2eT2qLVoAf5jkPU9W\nhHYN2vGP7v+gWZ1mJy7AEPPKK/Dww97fQgU1bAi16+9nX8UfqVwxh2q79lF91x6qWyTV2zSn0rkt\nsYoViIh07D77JeZwD3fF3sVLvV8iwiKOPKCISJjQKPQi5OVBTspuKjYIvFayHGnhQq8QzKWXQq9e\n3qQ4p50GTZpAxdop/GXxA0xOmEy9nErkHcxiV4FOjNqVa9O0dlOiIqKI3xbP2IvH8o/u/9CtChEJ\nCxqFfgwiMtOpeM6ZXiWTsWOPWP/astc4s/aZXHn2lSc+uBCxbZtXLKt5c29CnEPTjua5PN78/k3G\nvDeGfRl7GLM0kr9+E0m1ux9m74g72GR72LR7Ext3b2Tj7o1s2rOJxH2JPH3F09x/0f3B/VIiIuWE\nEnghXngBNn62lRf27fNKShVw4OABRs8dzbUtr1UCL0J2Ntx4o3ff+4svfkveK3as4M6P72RJ4hI6\nb6/IpA/yaH3lzbB6PJx6KrWAGNDsZSIixVACL8Ts2bBnRUVvJPMllxyx/r3V77H/4H6Gnzc8CNGF\nhrFj4csv4d13oVUr2J+1n0cXPsqL37zIKVkRvPUx3GItsRkTvTJeIiJSIkrghVi3Drpm/+DNzhV1\n5CV647s3aFmvpYpTAJv3bObVZa+ycfdGzAzDSFz6O7569k+c0+szPq70f8yZYSz4eT5b07YzYjn8\nc3lN6jzyT2/SlMjIYH8FEZGQpARewP793mNNLVlSaAWT1cmrWZK4hGd6PBO2A6mccyxe9TEvLBrP\nRzu/whycnemNPsva2Zxf3v4XlU79lpw2N7I0PhsHnLXzIHGfGhf0vQsmj4M6mm5UROR4KIEXsG6d\n97PlnV3g2iZHrE/cl0jzOs0Z0n7IiQ3sBHEOtmyB77//7fXjmhy6Nk/krrM/4rv0t3ih2moS6h6k\nTjo8sBzu2tKA02o2IS23Mr9b9xZ183L4ru7jnPbF2b8duHFjmP03aNcueF9OROQkosfICliwAEaO\nhJkz4dxzC9/GOXfStL6zsrxpw5cv95J1QsJvz2tHRMC5jfZyatJXLMzrRm5eZThnJmfGvsqDLatw\n8/m3UvX8C6B+fZyDm2+GKVO8AiOXXx7c7yUicjLQY2Ql0LUr/Pj0LKjSHjh8XvPEfYnUr1b/pKnx\nnZ7u1fqYNw8qV/ZqtVx/vVecq0MHaBn/Bq9OHc4jw4zcjPqcs+EZkhZfz+Z3+/L6+VCjGQw4BSrg\nTdby7rteARglbxGRsqeprApKT4frrvPqfxcwZMYQOv/nyPvioSgtzZsh9osv4N//9u79f/stvPaa\n1wNRbeH9XP7DcMZcAb1bXM36B7/kxxmD2Z5YkVdf9ba/6SZo1sx7VP7ee6F3b3jooWB/MxGR8KAE\nXsDll2YyLnusN3WYnw27NrBg8wKuPufqIEVWevbv92ZFW7TIq9UybNhvg+2zcw7yxF8v47y0Z9nU\noBLT+r3D9EEfcnYd7352lSrw+997c8XPnOkl8Cef9G5xH6r7IiIiZU9d6H6ys2FRQi06UeWIZ5P/\n/d2/ibAIhsYMDU5wpWTfPq+lvHSp1+V9442/rft+63KGvdKThAqpDExvxosPf0l0zSOLuICXqK++\n2nutXAl162pguYjIiaQE7ufnnyEnL5KWpx2A2rXzl2fnZvOfhP/Qp3kfGtdsHMQIj8/evdCzp1cy\ne9o0704BQFZOFn9b8BhPfjWe6EzHjLz+9HtyesB1sdu2LcOgRUSkUErgftauygUiaXlBrcOWz904\nl6S0JO44747gBFYKdu+GK6/0RpnHxXmD1/JcHh//9DEPzhvL6tQ13LoCnu30V+o8OC7Y4YqISDEC\numNpZj3N7Ecz22BmR1T2MLORZrbSzBLM7Esza+W37kHffj+a2ZWBHjMY1v7kzQrWYtxNhy3vdXYv\nFt66kN7NewcjrOO2a5c3MnzFCvjgA7iyTyb/Wv4vWk1sRd+pfUn7dSMfvwOTe0xS8hYRCRHFtsDN\nLBKYCFwBJALxZjbTObfGb7N3nXOv+rbvCzwL9PQl8oFAa+BUYJ6ZnePbp7hjnnBNmsANN0D1FodP\n4GJmXHbmZUGK6vikpsIVV3iDziZP2ct3NV5g2HMvk5KewnnptZjyiTFgXTZRb70LgwYFO1wREQlQ\nIF3onYANzrmNAGY2FbgGyE+2zrl9fttXAw7NDnMNMNU5lwVsMrMNvuNR3DGDYcj28Qzp0wj4bZa1\n55c+z6bdm3iu53NEWOgMsXYOZsyAe+6B1J15XP7QRG5f8wAZuZn02VKZP38Bl6VXxIaNgQ+Hw1ln\nBTtkEREpgUASeGPgV7/PicDvCm5kZncD9wEVgW5++y4tsO+hUWDFHvNEcrl55D71LFF9e8MQL4Hn\n5uXy3NLnOKfuOSGVvDdvhj/8wauqFn3WVrJu6c/c3OXcsjyP+5ZAq5hL4enfe0PIK54ck9KIiISb\nUstKzrmJzrlmwBjgL6V1XDMbYWbLzGxZSkpKaR32CFsXbaDqrl95t8Kt+cvmbpzLlr1bQqZsaHY2\nPPUUtG4NCxY4uvZ5hZTBZzB8Rzy/vHUKb7QaS6ulP3tznV53nZK3iEgIC6QFvhU4ze9zE9+yokwF\nXglg34CO6Zx7HXgdvLnQA4j3mKz96CeyOYdTO/9WgOON796gXtV6XHPuNWV12lKzZIk3wcrKlXDN\nOWs4p0E/JnRcz51bGjDx5hexfv2UsEVETiKBtMDjgeZm1tTMKuINSpvpv4GZNff72AdY73s/Exho\nZpXMrCnQHPg2kGOeaOu+3glAi+5eD3/SgSQ++vEjhrQbQqWoSsEM7ah27/YS90UXwZ6NO/kwagDd\nT2nNhO7ruaVRT15+Yxt2ww1K3iIiJ5liW+DOuRwzGwV8BkQCbzrnVpvZOGCZc24mMMrMLgeygd3A\nrb59V5vZe3iD03KAu51zuQCFHbP0v17g1u5sQO0KaTRoWA2AyIhIbmh9A8PPL7/d5/Hx0K93Fkk7\no7iP53k890ni7mnHPTWhf4v+vHn9eyF1715ERAKncqI+XbtCVpbj669Do0zo++/DkME5NMhO5IPa\nt3PefV2Iu6IxN34+nO5NuzNr0Kxy3XMgIiLFO1o5UTXPfK6/Hm6/3XDOcf9n97MqeVWwQyqUc/DE\n3/K44QY4L/sbvu35KOclzuSTgbHcNHckFza5kBk3zlDyFhE5yWkqVYD77uOujRvhww9554d3eXbp\ns7So14I29dsEO7LDZGbCHbfl8M7UKAbzX96463sqv/gmi379kmvfu5a2Ddry8U0fU61itWCHKiIi\nZUwtcGDvnK/Ytq86ezL28ue5f6ZT407cft7twQ7rMMnJ0L1zNu9MjeIJ/sL/PbeTyhOfIX7Hd1w9\n5Wqa1m7Kp4M/pVblWsUfTEREQp5a4Dt38sGPrRj2438YOuXvJB1IYtagWeVq8NeqVXBVz2ySt+Xw\nfoVbGTDteuI7NWHih0OZumoqp9Y4lbm3zCW6WnSwQxURkROk/GSpYPnyS9bSkgpRuby95XFGnD+C\n2FMLHS8QFHPmwEW/y+Hgtp3MPeVq0qY0o1PqP+n0Riemr53OsA7DWHzb4pAucyoiIiWnFvjixayL\n6Erzcxw3dHmYUZ1GBTuifB98ANcPyKNl5Eo6976Fay7Zxs5V82lRrwUv9XqJIe2HULNSzWCHKSIi\nQaAE3rYta2v+jg6to3i0y6PBjibfV//LY/CNudSqE8/qET1YWzmDfmf14+6Od9P1zK6YhcbjbiIi\nUjbCPoEn9evPhmHVueTULcDpwQ4HgB+/O8DVl+eRV20Hmbf15+Euo/j97+7mtFqnFb+ziIiEhfBO\n4Kmp/G3B49BvDz36PBjsaADYsWQTPbpUYH9kBSJuvYpZI9+he7PLgx2WiIiUM2E9iG3Fiw/xyqqX\nufPWKgy6olWww+HAR/Pp3XkPiXmn4G7px4ejXlDyFhGRQoVtAnfOcfeBadTc2oFr60wIdjBkj3+W\n6/pn8X1uO7hxEO/f+wC9mvcKblwiIlJuhW0Cn7l6Ol/V2sc5Xz/N8CFBnPwkPR03+GZGPFiTz11v\n7Kq7eWfsTfRv2T94MYmISLkXtgn86n2NeP89OJjRgZYtgxTE1q1wySU8NrUZk90d0Plv/GfchQxs\nMzBIAYmISKgI2wQe8eVXXLvG+DGpNi1aBCEA5+CWW3hjbSfGuXHQ/i0mPR3NrTG3BiEYEREJNeE7\nCv2669gScRYZoy04LfB33mHeAseIiJfhrM+Z8NJe7ux4TxACERGRUBS+CbxZM9a2bgZw4hP47t1k\n/ekBrq+5GBe1kUcmruTPl95/goMQEZFQFrZd6AAXXwwLFkBMzAk+8YMPcmO1gezZdzbX/ul/PN5T\nyVtEREomfFvgQM2a0KXLCT7pkiX8d14cH+3YQIP23/H+w7ed4ABERORkENYt8LffhsWLT+AJc3L4\nbuwQbqv1N8iuzsdvtSxXZUtFRCR0hHX2uP9++O9/T9z5Ep8fR88zKpPz/QiGDc/i/PZVTtzJRUTk\npBK2Xeipqd7rRA1gO7BxHVf9/Hd2ffk5NWs5nvp7tRNzYhEROSmFbQJfu9b7eSKeAc/Ny2XQy135\nYXcf3KbuPPEi1K1b9ucVEZGTV9gm8HXrvJ8nogV+/6v9mV1tJ9HzXqVeSxg5suzPKSIiJ7ewvQe+\nbh1UqQKnl3EJ8IlfPssLKbO49OMxpOw8lWefhQoVyvacIiJy8gvbBD5+vJfEI8rwCnyy/hPumfdn\neqyIZsVPj9K7N/TsWXbnExGR8BG2CTwqqmxb3wdzD3LHB0Npk+xosnYq6VlRPPNM2Z1PRETCS9gm\n8LI2ZcV/2ZaZzMgFFzD5p67cffeJGTAnIiLhQQm8DDjneHrWQ7TZAe9HzKB2beORR4IdlYiInEyU\nwMvAp5++zCqS6LbxzyxY05Bx46BOnWBHJSIiJ5OwfYyszGRk8PSssZxaJYpPN/6TVq3g978PdlAi\nInKyCagFbmY9zexHM9tgZmMLWX+fma0xsx/MbL6ZneFb3tXMEvxemWbWz7dusplt8lt3omuClYnv\n/noHXzRIp3vWC/y0IYpHH/UGzImIiJSmYhO4mUUCE4FeQCtgkJm1KrDZ90Csc64dEAc8BeCcW+Cc\ni3HOxQDdgHTgc7/9Rh9a75xLOP6vE2Tz5zNh87tUz63A6qUjaNYMrrsu2EGJiMjJKJAWeCdgg3Nu\no3PuIDAVuMZ/A1+iTvd9XAo0KeQ4A4BP/LY7uezezeZRN/N+a+hV+Tm+Wx7FAw9AZGSwAxMRkZNR\nIAm8MfCr3+dE37Ki3A58UsjygcCUAsv+7ut2f87MKhV2MDMbYWbLzGxZSkpKAOEGyahRPN80CYuM\nZPvc22nYEIYMCXZQIiJysirVUehmdjMQC0wosLwR0Bb4zG/xg0ALoCNQBxhT2DGdc68752Kdc7HR\n0dGlGW7pmTqV3R+8yxu/q0CPKg/z5cLK3HcfVK4c7MBERORkFUgC3wqc5ve5iW/ZYczscuBhoK9z\nLqvA6huAGc657EMLnHPbnScL+A9eV33o2boV7ryTV/s3IY2DZC/+E7VqaeS5iIiUrUASeDzQ3Mya\nmllFvK7wmf4bmFkH4DW85J1cyDEGUaD73Ncqx8wM6AesKnn4QZaXB7fdRlZuFi+2z+Liqrcx7+Pa\n3H031KwZ7OBERORkVmwCd87lAKPwur/XAu8551ab2Tgz6+vbbAJQHXjf90hYfoI3szPxWvCLChz6\nHTNbCawE6gFPHOd3OfEmToS5c/nv4wPYkZlCzWV/o1IluOeeYAcmIiInO3POBTuGgMXGxrply5YF\nOwzPrl3QpAl5XbvQpvdmbH8T1j/yGcOHGxMnBjs4ERE5GZjZcudcbGHrNJXqsfr0U8jIYM7I7qxN\nXcuZ654nL8/485+DHZiIiIQDJfBjNWsW1K/PhNSZNI5qw+LpLbnxRmjaNNiBiYhIOFACPxbZ2fDp\np3zbvxOLtyym3S+vcuCAMabQB+FERERKnxL4sfjqK9izh6db7KKmNeTbGRfSuze0axfswEREJFwo\ngR+L2bPZfkoFpu9bSqfkV9iZGsGDDwY7KBERCSdK4Mdi9mym921GXk4Ea2b25uKL4ZJLgh2UiIiE\nEyXwklq/Hn78kbjmOTTecj/bfq3I2CMKrIqIiJQtJfCSmj2bpGqwKPtnshf/iTZtoHfvYAclIiLh\nJirYAYScWbP4oHsjSIomeVMD/v4viNCfQSIicoIp9ZTEnj3wv/8R174i0TsGA9CrV5BjEhGRsKQW\neEl8/jnJlXJYGPkrZ/x6LfVbQ+OjVUYXEREpI2qBl8SsWXx4fjXysiuybfVZXHFFsAMSEZFwpQQe\nqNxcmDOHuAtqcuruG8nKjKBHj2AHJSIi4UoJPFBLlpCauYsvqiZxeuowKlaEzp2DHZSIiIQrJfBA\nzZ7NR60iyCWPXatiufhiqFYt2EGJiEi4UgIP1OzZvH9RbU6P7MhPa6rq/reIiASVEnggNm1i18bV\nzK+zh/bpfwJQAhcRkaBSAg/E7NnMPBdyyCNvwxXUrQsdOgQ7KBERCWd6DjwQs2bxfqdqnFErmu+/\nqkv37hAZGeygREQknKkFXpz9+9mzZAFzG2XQterdbNtmenxMRESCTi3w4sydy8xmOWQb1N5+PaD7\n3yIiEnxqgRdn1izi2kVxWs3T+PGb0znnHDj99GAHJSIi4U4J/Gjy8tj3+Sw+a5pHv7NvZNEidZ+L\niEj5oC70o4mPZ1bdnRyMgHMzhpKeru5zEREpH9QCP5pZs4hrDY2rNSLx+1ZERUGXLsEOSkRERAn8\nqPZ/+hGfNDcGtLmBefOMCy6AmjWDHZWIiIgSeNG2bOHjrFVkRTquaDSQ5cvR/W8RESk3lMCL8r//\n8X4raFQ5mv3rOuGc7n+LiEj5oQRehAPbNjOnOVzXoj/z5kZQqxbExgY7KhEREU9ACdzMeprZj2a2\nwczGFrL+PjNbY2Y/mNl8MzvDb12umSX4XjP9ljc1s298x5xmZhVL5yuVji92LSezAlzbdiBz50L3\n7hClMfsiIlJOFJvAzSwSmAj0AloBg8ysVYHNvgdinXPtgDjgKb91Gc65GN+rr9/y8cBzzrmzgd3A\n7cfxPUrdLwcSAai6vz1btqj7XEREypdAWuCdgA3OuY3OuYPAVOAa/w2ccwucc+m+j0uBJkc7oJkZ\n0A0v2QO8BfQrSeBlLSlzJ5F58O3i2oAGsImISPkSSAJvDPzq9znRt6wotwOf+H2ubGbLzGypmR1K\n0nWBPc65nACPecIl5+wlOqcS8+ZFcNZZcNZZwY5IRETkN6V6V9fMbgZigcv8Fp/hnNtqZmcBX5jZ\nSmBvCY45AhgBcPoJnIQ8mTSic2uyYAHcdNMJO62IiEhAAmmBbwVO8/vcxLfsMGZ2OfAw0Nc5l3Vo\nuXNuq+/nRmAh0AHYCdQ2s0N/QBR6TN9+rzvnYp1zsdHR0QGEWwqcIzkqi0pJl7F/v7rPRUSk/Akk\ngccDzX2jxisCA4GZ/huYWQfgNbzkney3/BQzq+R7Xw+4GFjjnHPAAmCAb9NbgY+O98uUmj17SKrq\nyPylBxER0K1bsAMSERE5XLEJ3HefehTwGbAWeM85t9rMxpnZoVHlE4DqwPsFHhdrCSwzsxV4CftJ\n59wa37oxwH1mtgHvnvi/S+1bHa+kJJKrwc6Nl9CpE9SuHeyAREREDhfQPXDn3BxgToFlj/i9v7yI\n/b4G2haxbiPeCPdyJ337Fg7k1SJty7ncMSTY0YiIiBxJM7EVInnbevilM85F6PlvEREpl5TAC5Gc\nvAn2eiPezz03yMGIiIgUQgm8EMm7EyE9GjNH3brBjkZERORISuCFSN63HdKiqX1KHpGRwY5GRETk\nSErghUjKSIX0aOpHW7BDERERKZQSeCGSD+4m8kB96tfX5RERkfJJGaoQyW4/Een1OVETv4mIiJSU\nEnghkiIycOn1qV8/2JGIiIgUTgm8oPR0kio6cjJOUQtcRETKLSXwgpKTSYqsAy5CCVxERMotJfAC\n8pJ2kOq8zK0ELiIi5ZUSeAE7t27AZSiBi4hI+RZQMZNwkpy8EdKUwEWkfMjOziYxMZHMzMxghyJl\nqHLlyjRp0oQKFSoEvI8SeAHJqVsg3cvcGoUuIsGWmJhIjRo1OPPMMzHT5FInI+ccO3fuJDExkaZN\nmwa8n7rQC0jeuxXSvMytedBFJNgyMzOpW7eukvdJzMyoW7duiXtZlMALSEpLzp8HvQQ9GSIiZUbJ\n++R3LP+NlcALSM7ciaVpHnQRESnflMALSM7dR4W0BkQrgYuISDmmBF5AEmmaB11ExE9kZCQxMTG0\nbt2a9u3b88wzz5CXlxe0eD788EPWrFlz1G2GDh1K06ZNiYmJoX379syfP7/Y406ePJlt27blf77j\njjuKPc/y5ctp27YtZ599Nvfccw/OuSO2+eijj2jXrh0xMTHExsby5ZdfFhtLIJTA/eXkkFzhIHnp\n0UrgIiI+VaqAOcNgAAAYQElEQVRUISEhgdWrVzN37lw++eQTHn/88SO2y8nJOSHxBJLAASZMmEBC\nQgLPP/88I0eOLHb7ggn8jTfeoFWrVkfd58477+Rf//oX69evZ/369Xz66adHbNO9e3dWrFhBQkIC\nb775JnfccUexsQRCCdxfaipJVY3sjFP0CJmIlD/33gtdupTu6957SxRC/fr1ef3113n55ZdxzjF5\n8mT69u1Lt27d6N69O845Ro8eTZs2bWjbti3Tpk0DYOHChXTu3Jk+ffpw7rnnMnLkyPxW/JQpU2jb\nti1t2rRhzJgx+eeqXr16/vu4uDiGDh3K119/zcyZMxk9ejQxMTH8/PPPxcZ84YUXsnXr1vzP48aN\no2PHjrRp04YRI0bgnCMuLo5ly5YxePBgYmJiyMjIoEuXLixbtqzIGLdv386+ffu44IILMDOGDBnC\nhx9+eMT5q1evnj9ILS0trdQGJSqB+0tOJjniFFxelFrgIiJFOOuss8jNzSU5ORmA7777jri4OBYt\nWsQHH3xAQkICK1asYN68eYwePZrt27cD8O233/LSSy+xZs0afv75Zz744AO2bdvGmDFj+OKLL0hI\nSCA+Pr7QJHjIRRddRN++ffNb182aNSs23k8//ZR+/frlfx41ahTx8fGsWrWKjIwMZs+ezYABA4iN\njeWdd94hISGBKlWq5G9fVIxbt26lSZMm+ds1adLksD8U/M2YMYMWLVrQp08f3nzzzWJjDoQmcvGT\nvu0X0nI0C5uIlFPPPx/sCAp1xRVXUKdOHQC+/PJLBg0aRGRkJA0aNOCyyy4jPj6emjVr0qlTJ846\n6ywABg0axJdffkmFChXo0qUL0b5/dAcPHszixYsPS7jHavTo0Tz00EMkJiayZMmS/OULFizgqaee\nIj09nV27dtG6dWuuvvrqIo8THx9faIw33XRTwLH079+f/v37s3jxYv76178yb968Y/9iPmqB+0ne\n8XP+LGxK4CIihdu4cSORkZHU991rrFatWkD7Few6Lq4r2X/9sUwlO2HCBH766SfGjx/PsGHD8o9z\n1113ERcXx8qVKxk+fPgxT1PbuHFjEhMT8z8nJibSuHHjo+7TuXNnNm7cSGpq6jGd058SuJ/klM2a\nB11E5ChSUlIYOXIko0aNKjQBX3rppUybNo3c3FxSUlJYvHgxnTp1Arwu9E2bNpGXl8e0adO45JJL\n6NSpE4sWLSI1NZXc3FymTJnCZZddBkCDBg1Yu3YteXl5zJgxI/8cNWrUYP/+/QHHPGrUKPLy8vjs\ns8/yk3W9evU4cOAAcXFxxR63qBgbNWpEzZo1Wbp0Kc453n77ba655poj9t+wYUP+6PTvvvuOrKws\n6pbCVJ/qQveTtPtXtcBFRArIyMggJiaG7OxsoqKiuOWWW7jvvvsK3bZ///4sWbKE9u3bY2Y89dRT\nNGzYkHXr1tGxY0dGjRrFhg0b6Nq1K/379yciIoInn3ySrl274pyjT58++UnwySef5KqrriI6OprY\n2FgOHDgAwMCBAxk+fDgvvvgicXFxxd4HNzP+8pe/8NRTTzF//nyGDx9OmzZtaNiwIR07dszfbujQ\noYwcOZIqVaoc1uXeqFGjImOcNGkSQ4cOJSMjg169etGrVy8AXn31VQBGjhzJ9OnTefvtt6lQoQJV\nqlRh2rRppTKQzQp7Zq28io2NdYdGBJaFf//hYu5Y3RUWPEFWFlSsWGanEhEJyNq1a2nZsmWwwzhu\nCxcu5Omnn2b27NnBDqXcKuy/tZktd87FFra9utD9JGekQno0tWo5JW8RESnX1IXuJzl7D1EHNI2q\niEhp69KlC126dCn1495999189dVXhy374x//yG233Vbq5ypvlMD9JOXt9+ZBbxTsSEREJBATJ04M\ndghBE1AXupn1NLMfzWyDmY0tZP19ZrbGzH4ws/lmdoZveYyZLTGz1b51N/rtM9nMNplZgu8VU3pf\n6xg4R3JkJpbeQAPYRESk3Cs2gZtZJDAR6AW0AgaZWcHJYb8HYp1z7YA44Cnf8nRgiHOuNdATeN7M\navvtN9o5F+N7JRzndzk+e/eSXNWRl1FPCVxERMq9QFrgnYANzrmNzrmDwFTgsAfdnHMLnHPpvo9L\ngSa+5T8559b73m8DkoHymR6Tk0mqCgfTT1ECFxGRci+QBN4Y+NXvc6JvWVFuBz4puNDMOgEVAf+Z\n5//u61p/zswqBRBLmcndsY2UyFrk5VZQIRMRESn3SvUxMjO7GYgFJhRY3gj4P+A259yhIrIPAi2A\njkAdYAyFMLMRZrbMzJalpKSUZriH2bV9Iy7dy9xqgYuI/Eb1wItW3uuBbwVO8/vcxLfsMGZ2OfAw\n0Nc5l+W3vCbwMfCwc27poeXOue3OkwX8B6+r/gjOudedc7HOudjoMsysyUkbNQubiEghVA+8aOW9\nHng80NzMmppZRWAgMNN/AzPrALyGl7yT/ZZXBGYAbzvn4grs08j304B+wKrj+SLHK2nnFs2DLiLl\nX2E1vSdN8talpxe+fvJkb31q6pHrSkj1wEOoHrhzLgcYBXwGrAXec86tNrNxZtbXt9kEoDrwvu+R\nsEMJ/gagMzC0kMfF3jGzlcBKoB7wRKl8o2OUvG+7WuAiIgFQPfAQqgfunJsDzCmw7BG/95cXsd9/\ngf8Wsa5b4GGWveS0ZEjzJrVXAheRcmvhwqLXVa169PX16h19/TFSPfDiqR54GUrO2oWl1adGDUfl\nysGORkSk/FI9cI/qgZcTSbn7qJSmedBFRI5G9cBVD7zcSbZ0otIaEq1nwEVEDqN64KoHftzKrB54\nZiYXjqrC6o9Xc1lsK2bNKv1TiIgcC9UDDx+qB34skpNJqg45mgddRERChLrQAZKSvHnQ0zQPuohI\nWVA98NKnBA6kbd9CuqsBORWUwEVEQojqgYe5lB0/50/iokImIiISCpTAgaTUzZpGVUREQooSOJC8\neyukqRKZiIiEDiVwIPnADs2DLiIiIUUJHEjK3KkudBGRIqgeeNECqQd+SHx8PFFRUYfN/nY8lMCB\n5Ow9VExrQNWqXi0AERH5jeqBFy2QeuAAubm5jBkzhh49ehQbR6D0GBmQ7A5QKa0hdTUCXUTKsXs/\nvZeEHQmlesyYhjE83/P5gLc/VA+8Y8eOPPbYY7z11lt88MEHHDhwgNzcXBYuXMgDDzzAJ598kj+F\n6Y033sjChQt55JFHqFGjRv5UqpMmTSIiIoIpU6bwj3/8I3+a0vHjxwNeHe1D06fGxcUxe/ZsRowY\nwcyZM1m0aBFPPPEE06dPL3Yq1cLqgc+aNYuMjAwuuugiXnvtNaZPn55fD/zQVKq9evXi6aefJjY2\nttAY/euBA/n1wA9Np+rvpZde4rrrriM+Pj7ga10ctcBzc0mOyiIyo6G6z0VEAqB64CWrB75161Zm\nzJjBnXfeWfzFLQG1wHfuJKkakN6A6HOCHYyISNFK0lI+kVQP/Ojuvfdexo8fT0RE6baZlcCTkkiu\nBjnpmgddRCQQoVQPfMCAAbz00ksMGzaM5cuX59cDX7ZsGaeddhqPPfZYmdcDX7ZsGQMHDgQgNTWV\nOXPmEBUVddx/pIR9F3pu0nZSqkBmWm0lcBGRYqgeeMnrgW/atInNmzezefNmBgwYwKRJk0qlhyHs\nE/iubRtxOdXIya6oBC4iUohD9cBbt27N5ZdfTo8ePXj00UcL3bZ///60a9eO9u3b061bt/x64EB+\nPfCWLVvStGlT+vfvf1it7fbt23P++ecfUQ/8oosuolGjRvnnGDhwIBMmTKBDhw78/PPPxcbvXw+8\ndu3a+fXAr7zyykLrgcfExJCRkZG//GgxTpo0iTvuuIOzzz6bZs2aHVYP/FBN8LIS9vXAVz0zhra/\nvgcvbOLNNyEMCtiISAhRPfDwoXrgJZS8a0v+JC4qZCIiIqEi7AexJe/drmlURUTKmOqBl76wT+BJ\nGSmQ1hRQAhcRCTWqBx7GkrN2EXFAlchERCS0hH0LPDlvP1XSGpJbGQJ8lFFERCTowjuBO0eyZVAx\nvRHVo6GYOQVERETKjfDuQt+/n6SqeURmNtQIdBERCSnhncB906i6jPq6/y0iUgTVAy+a6oEHS3Iy\nydUgO72uEriISBFUD7xowawHHtYJPG37L6RVhIwDmgddREJDl8ldjnhNip8EQHp2eqHrJydMBiA1\nPfWIdSV1qB74yy+/jHOOyZMn07dvX7p160b37t1xzjF69GjatGlD27ZtmTZtGuDNxNa5c2f69OnD\nueeey8iRI/Nb8VOmTKFt27a0adOGMWPG5J+revXq+e/j4uIYOnQoX3/9NTNnzmT06NHExMQENJVq\nYfXAO3bsSJs2bRgxYgTOOeLi4vLrgR+aSrVLly4cmv2zsBj964GbWX498MIcqgdevxTv1waUwM2s\np5n9aGYbzGxsIevvM7M1ZvaDmc03szP81t1qZut9r1v9lp9vZit9x3zRiitLUwaSkzbCwSpkZ1VS\nAhcRCZDqgYdIPXAziwQmAlcAiUC8mc10zvn3X3wPxDrn0s3sTuAp4EYzqwM8CsQCDlju23c38Aow\nHPgGmAP0BD4pva9WvOTUXzQLm4iElIVDFxa5rmqFqkddX69qvaOuP1aqB350ZVUPPJCjdQI2OOc2\nOucOAlOBw+qlOecWOOfSfR+XAof+JLkSmOuc2+VL2nOBnmbWCKjpnFvqvDv+bwPH/1+rhJL3bMuf\nB10JXEQkMKFUD/ynn35i/PjxDBs2LP84d911F3FxcaxcuZLhw4efsHrgZ555JnFxcdx1111H7WUI\nVCAJvDHwq9/nRN+yotzOby3povZt7Htf7DHNbISZLTOzZSkpKQGEG7ikAzvyW+B6jExEpHiqB36S\n1gM3s5vxussnlNYxnXOvO+dinXOx0aXcTE7O3KkWuIhIMVQPPETrgZvZhcBjzrkrfZ8fBHDO/bPA\ndpcDLwGXOeeSfcsGAV2cc7/3fX4NWOh7LXDOtShsu6KUdj3wewedwqupI8iaN569e6FmzVI7tIhI\nqVA98PBRFvXA44HmZtbUzCoCA4GZBU7QAXgN6Hsoeft8BvQws1PM7BSgB/CZc247sM/MLvCNPh8C\nfBTYVyw9SaRRJa0RFStCjRon+uwiIiLHrthR6M65HDMbhZeMI4E3nXOrzWwcsMw5NxOvy7w68L7v\nnsgW51xf59wuM/sb3h8BAOOcc7t87+8CJgNV8O6Zn9AR6GRlkVwxm4oZp1JN86CLiJQp1QMvfQEV\nM3HOzcF71Mt/2SN+7y8/yr5vAm8WsnwZ0CbgSEubbxa2iMyGuv8tIhKiVA88HPkSeF5mtEagi4hI\nyAnbBJ67YxupVTUPuoiIhKawTeA7d2wiLwIy9tdSAhcRkZATtgk8OXkTZFciM0PzoIuIHI3KiRYt\nkHKiCxcupFatWsTExBATE8O4ceOKjSUQ4ZvAdydqHnQRkQConGjRAi0neumll5KQkEBCQgKPPPJI\noduUVNgm8KR92zULm4iElHvvhS5dSvd1770li0HlRI+tnGhZCNsEnty3m1rgIiLHQOVES1ZOFGDJ\nkiW0b9+eXr16sXr16mJjDkRAz4GfjJIr5RKR0ZA8VMhERELD888HO4LCqZzo0Z133nn88ssvVK9e\nnTlz5tCvXz/Wr19/fF+OMG6BJ6UlUS37TEAtcBGRklA5UU+g5URr1qyZfzugd+/eZGdnk5qaekzn\n9Be2Cfyi0y6ibbWuVKgAtWoFOxoRkdCgcqIlLye6Y8eO/NHp3377LXl5edStWzfg+IsStl3owzoM\n4+uqsKme5kEXETmaQ+VEs7OziYqK4pZbbuG+++4rdNv+/fvn3+81s/xyouvWrcsvJ7phwwa6du1K\n//79iYiIyC/V6ZyjT58+R5QTjY6OJjY2lgMHDgBeOdHhw4fz4osvEhcXV+x9cP9yovPnz88vJ9qw\nYcNCy4lWqVLlsC53/3KiBWOcNGkSQ4cOJSMjg169eh1WThRg5MiRxMXF8corrxAVFUWVKlWYOnVq\nsb0PgSi2nGh5UtrlRK+5BjZvhhUrSu2QIiKlSuVEw0dZlBM9aaWk6P63iIiEprDtQgcvgfv1noiI\nSBlROdHSF/YJXC1wEZHQpXKiYejgQdi7VwlcRERCU9gm8EOP4CmBi4hIKArbBO6bAVAJXEREQlLY\nJvCUFO+nEriIiIQiJXAlcBGRo1I98KKpHngQHErgKmQiInJ0qgdeNNUDD4KUFIiMhNq1gx2JiEjg\nCqvpPWmSty49vfD1kyd761NTj1xXUqoHrnrgQZeSAvXqQUTYXgERkWOjeuCqBx5UzZrBlVcGOwoR\nkZJZuLDodVWrHn19vXpHX3+sVA/86FQPvJQ98AC89VawoxARCT2qB+5RPXAREQkZqgeueuAiIhIi\nVA9c9cCPW2nXAxcRKe9UDzx8qB64iIhIGFAXuoiIlDnVAy99ASVwM+sJvABEAm84554ssL4z8DzQ\nDhjonIvzLe8KPOe3aQvf+g/NbDJwGbDXt26ocy7hOL6LiIiEmXCuB15sAjezSGAicAWQCMSb2Uzn\nnP88dluAocCf/fd1zi0AYnzHqQNsAD7322T0oWQvIiKFc86VyqAnKb+OZTxaIPfAOwEbnHMbnXMH\nganAYePknXObnXM/AEeb3X4A8IlzLr3EUYqIhKnKlSuzc+fOY/oHXkKDc46dO3dSuXLlEu0XSBd6\nY+BXv8+JwO9KdBbPQODZAsv+bmaPAPOBsc65rII7mdkIYATA6aeffgynFREJXU2aNCExMZGUQxWY\n5KRUuXLlw6ZlDcQJGcRmZo2AtsBnfosfBHYAFYHXgTHAETXWnHOv+9YTGxurP0FFJKxUqFCBpk2b\nBjsMKYcC6ULfCpzm97mJb1lJ3ADMcM5lH1rgnNvuPFnAf/C66kVERCQAgSTweKC5mTU1s4p4XeEz\nS3ieQcAU/wW+VjnmjczoB6wq4TFFRETCVrEJ3DmXA4zC6/5eC7znnFttZuPMrC+AmXU0s0TgeuA1\nM8uvlWZmZ+K14BcVOPQ7ZrYSWAnUA544/q8jIiISHkJqKlUzSwF+KcVD1gOOvySMgK5ladK1LD26\nlqVD17H0lPRanuGciy5sRUgl8NJmZsuKmmNWSkbXsvToWpYeXcvSoetYekrzWmoudBERkRCkBC4i\nIhKCwj2Bvx7sAE4iupalR9ey9Ohalg5dx9JTatcyrO+Bi4iIhKpwb4GLiIiEpLBN4GbW08x+NLMN\nZjY22PGEEjN708ySzWyV37I6ZjbXzNb7fp4SzBhDgZmdZmYLzGyNma02sz/6lutalpCZVTazb81s\nhe9aPu5b3tTMvvH9nk/zTUYlATCzSDP73sxm+z7rWh4DM9tsZivNLMHMlvmWlcrveFgmcL8Sqb2A\nVsAgM2sV3KhCymSgZ4FlY4H5zrnm+IrTnOigQlAOcL9zrhVwAXC37/9DXcuSywK6Oefa45Uw7mlm\nFwDjgeecc2cDu4HbgxhjqPkj3uRdh+haHruuzrkYv8fHSuV3PCwTOAGUSJWiOecWA7sKLL4GeMv3\n/i286XHlKHz1AL7zvd+P949lY3QtS8xXV+GA72MF38sB3YA433JdywCZWROgD/CG77Oha1maSuV3\nPFwTeGElUhsHKZaTRQPn3Hbf+x1Ag2AGE2p8Uw53AL5B1/KY+Lp8E4BkYC7wM7DHNx006Pe8JJ4H\nHgDyfJ/romt5rBzwuZkt95XHhlL6HT8h5UQlvDjnnJnp8YYAmVl1YDpwr3Nun9fY8ehaBs45lwvE\nmFltYAbQIsghhSQzuwpIds4tN7MuwY7nJHCJc26rmdUH5prZOv+Vx/M7Hq4t8NIokSqHS/KrMNcI\nrxUkxTCzCnjJ+x3n3Ae+xbqWx8E5twdYAFwI1DazQw0V/Z4H5mKgr5ltxru92A14AV3LY+Kc2+r7\nmYz3h2UnSul3PFwTeGmUSJXDzQRu9b2/FfgoiLGEBN99xX8Da51zz/qt0rUsITOL9rW8MbMqwBV4\nYwoWAAN8m+laBsA596Bzrolz7ky8fxu/cM4NRteyxMysmpnVOPQe6IFXOrtUfsfDdiIXM+uNd58n\nEnjTOff3IIcUMsxsCtAFr6pOEvAo8CHwHnA6XsW4G5xzBQe6iR8zuwT4H15J3UP3Gh/Cuw+ua1kC\nZtYObzBQJF7D5D3n3DgzOwuvFVkH+B642TmXFbxIQ4uvC/3PzrmrdC1LznfNZvg+RgHvOuf+bmZ1\nKYXf8bBN4CIiIqEsXLvQRUREQpoSuIiISAhSAhcREQlBSuAiIiIhSAlcREQkBCmBi4iIhCAlcBER\nkRCkBC4iIhKC/h+izJOHd7bqHgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PrnOMJdBqJN",
        "colab_type": "text"
      },
      "source": [
        "Drop out ratio=0.3 表現最好"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI8ROXKx7oEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}